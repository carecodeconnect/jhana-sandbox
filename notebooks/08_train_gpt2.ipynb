{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow in /home/solaris/miniconda3/envs/jhana_ai/lib/python3.10/site-packages (2.15.0.post1)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /home/solaris/miniconda3/envs/jhana_ai/lib/python3.10/site-packages (from tensorflow) (2.1.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /home/solaris/miniconda3/envs/jhana_ai/lib/python3.10/site-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=23.5.26 in /home/solaris/miniconda3/envs/jhana_ai/lib/python3.10/site-packages (from tensorflow) (23.5.26)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /home/solaris/miniconda3/envs/jhana_ai/lib/python3.10/site-packages (from tensorflow) (0.5.4)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /home/solaris/miniconda3/envs/jhana_ai/lib/python3.10/site-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: h5py>=2.9.0 in /home/solaris/miniconda3/envs/jhana_ai/lib/python3.10/site-packages (from tensorflow) (3.10.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /home/solaris/miniconda3/envs/jhana_ai/lib/python3.10/site-packages (from tensorflow) (16.0.6)\n",
      "Requirement already satisfied: ml-dtypes~=0.2.0 in /home/solaris/miniconda3/envs/jhana_ai/lib/python3.10/site-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in /home/solaris/miniconda3/envs/jhana_ai/lib/python3.10/site-packages (from tensorflow) (1.26.4)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /home/solaris/miniconda3/envs/jhana_ai/lib/python3.10/site-packages (from tensorflow) (3.3.0)\n",
      "Requirement already satisfied: packaging in /home/solaris/miniconda3/envs/jhana_ai/lib/python3.10/site-packages (from tensorflow) (23.2)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /home/solaris/miniconda3/envs/jhana_ai/lib/python3.10/site-packages (from tensorflow) (4.25.2)\n",
      "Requirement already satisfied: setuptools in /home/solaris/miniconda3/envs/jhana_ai/lib/python3.10/site-packages (from tensorflow) (69.0.3)\n",
      "Requirement already satisfied: six>=1.12.0 in /home/solaris/miniconda3/envs/jhana_ai/lib/python3.10/site-packages (from tensorflow) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /home/solaris/miniconda3/envs/jhana_ai/lib/python3.10/site-packages (from tensorflow) (2.4.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /home/solaris/miniconda3/envs/jhana_ai/lib/python3.10/site-packages (from tensorflow) (4.9.0)\n",
      "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /home/solaris/miniconda3/envs/jhana_ai/lib/python3.10/site-packages (from tensorflow) (1.14.1)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /home/solaris/miniconda3/envs/jhana_ai/lib/python3.10/site-packages (from tensorflow) (0.36.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /home/solaris/miniconda3/envs/jhana_ai/lib/python3.10/site-packages (from tensorflow) (1.60.1)\n",
      "Requirement already satisfied: tensorboard<2.16,>=2.15 in /home/solaris/miniconda3/envs/jhana_ai/lib/python3.10/site-packages (from tensorflow) (2.15.2)\n",
      "Requirement already satisfied: tensorflow-estimator<2.16,>=2.15.0 in /home/solaris/miniconda3/envs/jhana_ai/lib/python3.10/site-packages (from tensorflow) (2.15.0)\n",
      "Requirement already satisfied: keras<2.16,>=2.15.0 in /home/solaris/miniconda3/envs/jhana_ai/lib/python3.10/site-packages (from tensorflow) (2.15.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /home/solaris/miniconda3/envs/jhana_ai/lib/python3.10/site-packages (from astunparse>=1.6.0->tensorflow) (0.42.0)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /home/solaris/miniconda3/envs/jhana_ai/lib/python3.10/site-packages (from tensorboard<2.16,>=2.15->tensorflow) (2.27.0)\n",
      "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in /home/solaris/miniconda3/envs/jhana_ai/lib/python3.10/site-packages (from tensorboard<2.16,>=2.15->tensorflow) (1.2.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /home/solaris/miniconda3/envs/jhana_ai/lib/python3.10/site-packages (from tensorboard<2.16,>=2.15->tensorflow) (3.5.2)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /home/solaris/miniconda3/envs/jhana_ai/lib/python3.10/site-packages (from tensorboard<2.16,>=2.15->tensorflow) (2.31.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /home/solaris/miniconda3/envs/jhana_ai/lib/python3.10/site-packages (from tensorboard<2.16,>=2.15->tensorflow) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /home/solaris/miniconda3/envs/jhana_ai/lib/python3.10/site-packages (from tensorboard<2.16,>=2.15->tensorflow) (3.0.1)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /home/solaris/miniconda3/envs/jhana_ai/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (5.3.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /home/solaris/miniconda3/envs/jhana_ai/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (0.3.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /home/solaris/miniconda3/envs/jhana_ai/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (4.9)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /home/solaris/miniconda3/envs/jhana_ai/lib/python3.10/site-packages (from google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow) (1.3.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/solaris/miniconda3/envs/jhana_ai/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/solaris/miniconda3/envs/jhana_ai/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/solaris/miniconda3/envs/jhana_ai/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (2.2.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/solaris/miniconda3/envs/jhana_ai/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (2024.2.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /home/solaris/miniconda3/envs/jhana_ai/lib/python3.10/site-packages (from werkzeug>=1.0.1->tensorboard<2.16,>=2.15->tensorflow) (2.1.5)\n",
      "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /home/solaris/miniconda3/envs/jhana_ai/lib/python3.10/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (0.5.1)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /home/solaris/miniconda3/envs/jhana_ai/lib/python3.10/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow) (3.2.2)\n",
      "Requirement already satisfied: transformers==4.33.0 in /home/solaris/miniconda3/envs/jhana_ai/lib/python3.10/site-packages (4.33.0)\n",
      "Requirement already satisfied: tokenizers in /home/solaris/miniconda3/envs/jhana_ai/lib/python3.10/site-packages (0.13.3)\n",
      "Requirement already satisfied: datasets in /home/solaris/miniconda3/envs/jhana_ai/lib/python3.10/site-packages (2.17.0)\n",
      "Requirement already satisfied: accelerate in /home/solaris/miniconda3/envs/jhana_ai/lib/python3.10/site-packages (0.27.2)\n",
      "Requirement already satisfied: filelock in /home/solaris/miniconda3/envs/jhana_ai/lib/python3.10/site-packages (from transformers==4.33.0) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.15.1 in /home/solaris/miniconda3/envs/jhana_ai/lib/python3.10/site-packages (from transformers==4.33.0) (0.20.3)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/solaris/miniconda3/envs/jhana_ai/lib/python3.10/site-packages (from transformers==4.33.0) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/solaris/miniconda3/envs/jhana_ai/lib/python3.10/site-packages (from transformers==4.33.0) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/solaris/miniconda3/envs/jhana_ai/lib/python3.10/site-packages (from transformers==4.33.0) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/solaris/miniconda3/envs/jhana_ai/lib/python3.10/site-packages (from transformers==4.33.0) (2023.12.25)\n",
      "Requirement already satisfied: requests in /home/solaris/miniconda3/envs/jhana_ai/lib/python3.10/site-packages (from transformers==4.33.0) (2.31.0)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /home/solaris/miniconda3/envs/jhana_ai/lib/python3.10/site-packages (from transformers==4.33.0) (0.4.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/solaris/miniconda3/envs/jhana_ai/lib/python3.10/site-packages (from transformers==4.33.0) (4.66.2)\n",
      "Requirement already satisfied: pyarrow>=12.0.0 in /home/solaris/miniconda3/envs/jhana_ai/lib/python3.10/site-packages (from datasets) (15.0.0)\n",
      "Requirement already satisfied: pyarrow-hotfix in /home/solaris/miniconda3/envs/jhana_ai/lib/python3.10/site-packages (from datasets) (0.6)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /home/solaris/miniconda3/envs/jhana_ai/lib/python3.10/site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in /home/solaris/miniconda3/envs/jhana_ai/lib/python3.10/site-packages (from datasets) (1.5.3)\n",
      "Requirement already satisfied: xxhash in /home/solaris/miniconda3/envs/jhana_ai/lib/python3.10/site-packages (from datasets) (3.4.1)\n",
      "Requirement already satisfied: multiprocess in /home/solaris/miniconda3/envs/jhana_ai/lib/python3.10/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2023.10.0,>=2023.1.0 in /home/solaris/miniconda3/envs/jhana_ai/lib/python3.10/site-packages (from fsspec[http]<=2023.10.0,>=2023.1.0->datasets) (2023.10.0)\n",
      "Requirement already satisfied: aiohttp in /home/solaris/miniconda3/envs/jhana_ai/lib/python3.10/site-packages (from datasets) (3.9.3)\n",
      "Requirement already satisfied: psutil in /home/solaris/miniconda3/envs/jhana_ai/lib/python3.10/site-packages (from accelerate) (5.9.8)\n",
      "Requirement already satisfied: torch>=1.10.0 in /home/solaris/miniconda3/envs/jhana_ai/lib/python3.10/site-packages (from accelerate) (2.2.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/solaris/miniconda3/envs/jhana_ai/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/solaris/miniconda3/envs/jhana_ai/lib/python3.10/site-packages (from aiohttp->datasets) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/solaris/miniconda3/envs/jhana_ai/lib/python3.10/site-packages (from aiohttp->datasets) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/solaris/miniconda3/envs/jhana_ai/lib/python3.10/site-packages (from aiohttp->datasets) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /home/solaris/miniconda3/envs/jhana_ai/lib/python3.10/site-packages (from aiohttp->datasets) (1.9.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /home/solaris/miniconda3/envs/jhana_ai/lib/python3.10/site-packages (from aiohttp->datasets) (4.0.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/solaris/miniconda3/envs/jhana_ai/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.15.1->transformers==4.33.0) (4.9.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/solaris/miniconda3/envs/jhana_ai/lib/python3.10/site-packages (from requests->transformers==4.33.0) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/solaris/miniconda3/envs/jhana_ai/lib/python3.10/site-packages (from requests->transformers==4.33.0) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/solaris/miniconda3/envs/jhana_ai/lib/python3.10/site-packages (from requests->transformers==4.33.0) (2.2.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/solaris/miniconda3/envs/jhana_ai/lib/python3.10/site-packages (from requests->transformers==4.33.0) (2024.2.2)\n",
      "Requirement already satisfied: sympy in /home/solaris/miniconda3/envs/jhana_ai/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (1.12)\n",
      "Requirement already satisfied: networkx in /home/solaris/miniconda3/envs/jhana_ai/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (2.8.8)\n",
      "Requirement already satisfied: jinja2 in /home/solaris/miniconda3/envs/jhana_ai/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.1.3)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /home/solaris/miniconda3/envs/jhana_ai/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /home/solaris/miniconda3/envs/jhana_ai/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /home/solaris/miniconda3/envs/jhana_ai/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /home/solaris/miniconda3/envs/jhana_ai/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /home/solaris/miniconda3/envs/jhana_ai/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /home/solaris/miniconda3/envs/jhana_ai/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /home/solaris/miniconda3/envs/jhana_ai/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /home/solaris/miniconda3/envs/jhana_ai/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /home/solaris/miniconda3/envs/jhana_ai/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /home/solaris/miniconda3/envs/jhana_ai/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (2.19.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /home/solaris/miniconda3/envs/jhana_ai/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
      "Requirement already satisfied: triton==2.2.0 in /home/solaris/miniconda3/envs/jhana_ai/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (2.2.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /home/solaris/miniconda3/envs/jhana_ai/lib/python3.10/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.10.0->accelerate) (12.3.101)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /home/solaris/miniconda3/envs/jhana_ai/lib/python3.10/site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/solaris/miniconda3/envs/jhana_ai/lib/python3.10/site-packages (from pandas->datasets) (2024.1)\n",
      "Requirement already satisfied: six>=1.5 in /home/solaris/miniconda3/envs/jhana_ai/lib/python3.10/site-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/solaris/miniconda3/envs/jhana_ai/lib/python3.10/site-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.5)\n",
      "Requirement already satisfied: mpmath>=0.19 in /home/solaris/miniconda3/envs/jhana_ai/lib/python3.10/site-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow  # This will install TensorFlow along with a compatible numpy version\n",
    "!pip install transformers==4.33.0 tokenizers datasets accelerate\n",
    "# Install tts with a version compatible with the numpy version required by TensorFlow, if available\n",
    "# !pip install tts==compatible_version_here\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-18 15:19:53.222595: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-02-18 15:19:53.222643: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-02-18 15:19:53.267436: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-02-18 15:19:53.365949: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-02-18 15:19:54.470251: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "/home/solaris/miniconda3/envs/jhana_ai/lib/python3.10/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n",
      "/home/solaris/miniconda3/envs/jhana_ai/lib/python3.10/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n",
      "2024-02-18 15:20:00.418571: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-02-18 15:20:00.422942: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-02-18 15:20:00.423055: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import glob\n",
    "import os\n",
    "import shutil\n",
    "import tqdm\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import BPE\n",
    "from tokenizers.trainers import BpeTrainer\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "from transformers import PreTrainedTokenizerFast\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "from transformers import GPT2Config, GPT2LMHeadModel\n",
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "tf.config.list_physical_devices(\"GPU\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import os\n",
    "import re\n",
    "import glob\n",
    "\n",
    "# Define the source and target directories\n",
    "source_dir = '../data/input/clean_data'\n",
    "target_dir = '../data/input/filtered_data'\n",
    "dataset_file = os.path.join(target_dir, \"filtered_dataset.txt\")\n",
    "\n",
    "# Ensure the target directory exists\n",
    "os.makedirs(target_dir, exist_ok=True)\n",
    "\n",
    "# Regex pattern to keep English letters, Pali diacritics, and standard punctuation\n",
    "pattern = re.compile(r\"[^a-zA-ZāīūṅñṭḍṇḷĀĪŪṄÑṬḌṆḶ\\s.,!?()-]\")\n",
    "\n",
    "# Function to clean text by removing unwanted characters\n",
    "def clean_text(text):\n",
    "    return pattern.sub('', text)\n",
    "\n",
    "# Find all .txt files in the source directory\n",
    "paths = glob.glob(os.path.join(source_dir, \"*.txt\"))[:1000]  # Limit to first 1000 files\n",
    "\n",
    "# Process and merge content of each file\n",
    "with open(dataset_file, \"w\", encoding='utf-8') as output_file:\n",
    "    for path in paths:\n",
    "        with open(path, \"r\", encoding='utf-8') as file:\n",
    "            for line in file:\n",
    "                cleaned_line = clean_text(line.strip())\n",
    "                if cleaned_line:\n",
    "                    print(cleaned_line, file=output_file)\n",
    "\n",
    "print(\"Corpus loaded and filtered.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import os\n",
    "import re\n",
    "import glob\n",
    "import shutil\n",
    "\n",
    "# Define the source directories for copying .txt files\n",
    "source_dirs = [\n",
    "    \"../data/input/text/books_converted\",\n",
    "    \"../data/input/text/web_txt\",\n",
    "    \"../data/input/audio_transcribed/transcribed_youtube\",\n",
    "    \"../data/input/audio_transcribed/transscribed_audio_dharma\"\n",
    "]\n",
    "\n",
    "# Define the target directory for copied .txt files\n",
    "copy_target_dir = '../data/input/clean_data'\n",
    "\n",
    "# Ensure the target directory exists\n",
    "os.makedirs(copy_target_dir, exist_ok=True)\n",
    "\n",
    "# Copy all .txt files from the source directories to the target directory\n",
    "for src_dir in source_dirs:\n",
    "    for txt_file in glob.glob(os.path.join(src_dir, \"*.txt\")):\n",
    "        shutil.copy(txt_file, copy_target_dir)\n",
    "\n",
    "# Now, set the source_dir to where we've copied the .txt files\n",
    "source_dir = copy_target_dir\n",
    "\n",
    "# Define the target directory for preprocessing output\n",
    "target_dir = '../data/input/filtered_data'\n",
    "dataset_file = os.path.join(target_dir, \"filtered_dataset.txt\")\n",
    "\n",
    "# Ensure the preprocessing target directory exists\n",
    "os.makedirs(target_dir, exist_ok=True)\n",
    "\n",
    "# Regex pattern to keep English letters, Pali diacritics, and standard punctuation\n",
    "pattern = re.compile(r\"[^a-zA-ZāīūṅñṭḍṇḷĀĪŪṄÑṬḌṆḶ\\s.,!?()-]\")\n",
    "\n",
    "# Function to clean text by removing unwanted characters\n",
    "def clean_text(text):\n",
    "    return pattern.sub('', text)\n",
    "\n",
    "# Find all .txt files in the source directory for preprocessing\n",
    "paths = glob.glob(os.path.join(source_dir, \"*.txt\"))[:1000]  # Limit to first 1000 files\n",
    "\n",
    "# Process and merge content of each file\n",
    "with open(dataset_file, \"w\", encoding='utf-8') as output_file:\n",
    "    for path in paths:\n",
    "        with open(path, \"r\", encoding='utf-8') as file:\n",
    "            for line in file:\n",
    "                cleaned_line = clean_text(line.strip())\n",
    "                if cleaned_line:\n",
    "                    print(cleaned_line, file=output_file)\n",
    "\n",
    "print(\"Corpus loaded and filtered.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean Dataset: Convert to English Sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import spacy\n",
    "import re\n",
    "\n",
    "# Load the English tokenizer, tagger, parser, NER, and word vectors\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def is_likely_english(sentence):\n",
    "    # Check if the sentence has at least one verb and one subject, common in English sentences\n",
    "    has_verb = any(token.pos_ == 'VERB' for token in sentence)\n",
    "    has_subject = any(token.dep_ == 'nsubj' or token.dep_ == 'nsubjpass' for token in sentence)\n",
    "    return has_verb and has_subject\n",
    "\n",
    "def is_not_reference(line):\n",
    "    # Filter out lines that resemble references or citations, e.g., \"SN V,\" \"T II b\"\n",
    "    return not re.search(r'\\b(SN|T|MN|AN|D|SĀ|Ps|Vin|Vism)\\s+[IVXLC]+', line)\n",
    "\n",
    "def filter_sentences(input_file, output_file):\n",
    "    with open(input_file, 'r', encoding='utf-8') as f_in, open(output_file, 'w', encoding='utf-8') as f_out:\n",
    "        for line in f_in:\n",
    "            if is_not_reference(line):\n",
    "                doc = nlp(line.strip())\n",
    "                for sent in doc.sents:\n",
    "                    if is_likely_english(sent) and len(sent.text.split()) > 5:  # Minimum word count check\n",
    "                        f_out.write(sent.text + '\\n')\n",
    "\n",
    "# Specify the path to your input file and the desired output file\n",
    "input_file = '../data/input/filtered_data/filtered_dataset.txt'\n",
    "output_file = '../data/input/filtered_data/cleaned_dataset.txt'\n",
    "\n",
    "# Filter the dataset\n",
    "filter_sentences(input_file, output_file)\n",
    "\n",
    "print(\"Dataset filtering complete. Check the cleaned dataset at:\", output_file)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combined Cleaning and Filtering of Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import glob\n",
    "import shutil\n",
    "import spacy\n",
    "\n",
    "# Define the source directories for copying .txt files\n",
    "source_dirs = [\n",
    "    \"../data/input/text/books_converted\",\n",
    "    \"../data/input/text/web_txt\",\n",
    "    \"../data/input/audio_transcribed/transcribed_youtube\",\n",
    "    \"../data/input/audio_transcribed/transscribed_audio_dharma\"\n",
    "]\n",
    "\n",
    "# Define the first target directory for copied .txt files\n",
    "copy_target_dir = '../data/input/clean_data'\n",
    "\n",
    "# Ensure the first target directory exists\n",
    "os.makedirs(copy_target_dir, exist_ok=True)\n",
    "\n",
    "# Copy all .txt files from the source directories to the first target directory\n",
    "for src_dir in source_dirs:\n",
    "    for txt_file in glob.glob(os.path.join(src_dir, \"*.txt\")):\n",
    "        shutil.copy(txt_file, copy_target_dir)\n",
    "\n",
    "# Define the target directory for preprocessing output and the output file\n",
    "target_dir = '../data/input/filtered_data'\n",
    "dataset_file = os.path.join(target_dir, \"filtered_dataset.txt\")\n",
    "\n",
    "# Ensure the preprocessing target directory exists\n",
    "os.makedirs(target_dir, exist_ok=True)\n",
    "\n",
    "# Regex pattern to keep English letters, Pali diacritics, and standard punctuation\n",
    "pattern = re.compile(r\"[^a-zA-ZāīūṅñṭḍṇḷĀĪŪṄÑṬḌṆḶ\\s.,!?()-]\")\n",
    "\n",
    "# Function to clean text by removing unwanted characters\n",
    "def clean_text(text):\n",
    "    return pattern.sub('', text)\n",
    "\n",
    "# Find and preprocess .txt files in the first target directory\n",
    "paths = glob.glob(os.path.join(copy_target_dir, \"*.txt\"))[:1000]  # Limit to first 1000 files\n",
    "\n",
    "# Process and merge content of each file\n",
    "with open(dataset_file, \"w\", encoding='utf-8') as output_file:\n",
    "    for path in paths:\n",
    "        with open(path, \"r\", encoding='utf-8') as file:\n",
    "            for line in file:\n",
    "                cleaned_line = clean_text(line.strip())\n",
    "                if cleaned_line:\n",
    "                    print(cleaned_line, file=output_file)\n",
    "\n",
    "# Load the English tokenizer, tagger, parser, NER, and word vectors from spaCy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def is_likely_english(sentence):\n",
    "    # Check if the sentence has at least one verb and one subject\n",
    "    has_verb = any(token.pos_ == 'VERB' for token in sentence)\n",
    "    has_subject = any(token.dep_ == 'nsubj' or token.dep_ == 'nsubjpass' for token in sentence)\n",
    "    return has_verb and has_subject\n",
    "\n",
    "def is_not_reference(line):\n",
    "    # Filter out lines that resemble references or citations\n",
    "    return not re.search(r'\\b(SN|T|MN|AN|D|SĀ|Ps|Vin|Vism)\\s+[IVXLC]+', line)\n",
    "\n",
    "def filter_sentences(input_file, output_file):\n",
    "    with open(input_file, 'r', encoding='utf-8') as f_in, open(output_file, 'w', encoding='utf-8') as f_out:\n",
    "        for line in f_in:\n",
    "            if is_not_reference(line):\n",
    "                doc = nlp(line.strip())\n",
    "                for sent in doc.sents:\n",
    "                    if is_likely_english(sent) and len(sent.text.split()) > 5:  # Minimum word count check\n",
    "                        f_out.write(sent.text + '\\n')\n",
    "\n",
    "# Specify the path for the final output file\n",
    "final_output_file = '../data/input/filtered_data/cleaned_dataset.txt'\n",
    "\n",
    "# Filter the preprocessed dataset\n",
    "filter_sentences(dataset_file, final_output_file)\n",
    "\n",
    "print(\"Corpus loaded, filtered, and cleaned. Check the cleaned dataset at:\", final_output_file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Produce Descriptive Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Words: 9_171_086\n",
      "Unique Words: 85_247\n",
      "Average Word Length: 5.37 characters\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def format_number(number):\n",
    "    \"\"\"Format the number with underscores between hundreds.\"\"\"\n",
    "    return f\"{number:,}\".replace(\",\", \"_\")\n",
    "\n",
    "def count_words(file_path):\n",
    "    \"\"\"Count words in a file and provide descriptive statistics.\"\"\"\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            text = file.read()\n",
    "            # Tokenize the text by non-alphanumeric characters\n",
    "            words = re.findall(r'\\b\\w+\\b', text.lower())\n",
    "            word_count = len(words)\n",
    "            unique_words = len(set(words))\n",
    "            average_word_length = sum(len(word) for word in words) / word_count\n",
    "\n",
    "            print(f\"Total Words: {format_number(word_count)}\")\n",
    "            print(f\"Unique Words: {format_number(unique_words)}\")\n",
    "            print(f\"Average Word Length: {average_word_length:.2f} characters\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"The file at {file_path} was not found.\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    file_path = \"../data/input/filtered_data/cleaned_dataset.txt\"\n",
    "    count_words(file_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 346626\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_dir = '../data/input/filtered_data'\n",
    "dataset_file = os.path.join(target_dir, \"cleaned_dataset.txt\")\n",
    "\n",
    "raw_datasets = load_dataset(\"text\", data_files=dataset_file)\n",
    "raw_datasets\n",
    "# 333_056 lines\n",
    "# previously around 43_000 lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': 'Im assuming everybody is familiar with them.'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_datasets[\"train\"][666]\n",
    "# with HuggingFace datasets, every sample is a dictionary\n",
    "# a sample from the dataset here is a paragraph\n",
    "# the key is always \"text\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create empty tokenizer and its trainer\n",
    "tokenizer = Tokenizer(BPE(unk_token=\"[UNK]\")) # subword tokenization and ways to merge them\n",
    "trainer = BpeTrainer(vocab_size=5_000, special_tokens=[\"[UNK]\", \"[PAD]\"])\n",
    "# separates the tokens with a space\n",
    "tokenizer.pre_tokenizer = Whitespace()\n",
    "\n",
    "# Batch samples to speed up process\n",
    "def batch_iterator(batch_size=1000):\n",
    "    # the batch size is the number of samples that will be processed at once\n",
    "    # the iterator will yield a batch of samples\n",
    "    # yield is a keyword in Python that is used like return, except the function will return a generator\n",
    "    # a generator is an iterator that generates one item at a time\n",
    "    for i in range(0, len(raw_datasets[\"train\"]), batch_size):\n",
    "        yield raw_datasets[\"train\"][i : i + batch_size][\"text\"]\n",
    "\n",
    "# Train the tokenizer\n",
    "tokenizer.train_from_iterator(batch_iterator(), trainer=trainer, length=len(raw_datasets[\"train\"]))\n",
    "\n",
    "# Saves the tokenizer\n",
    "# when downloading model, we download model, and the tokenizer\n",
    "tokenizer.save(\"tokenizer.json\")\n",
    "\n",
    "# Load it fast\n",
    "# speeds up the process\n",
    "tokenizer = PreTrainedTokenizerFast(tokenizer_file=\"tokenizer.json\") \n",
    "tokenizer.add_special_tokens({\"pad_token\": \"[PAD]\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenize the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start with sequence length of 256\n",
    "# pads the sequences to the same length\n",
    "sequence_length = 256\n",
    "\n",
    "# takes a dictionary as input\n",
    "def tokenize_function(example):\n",
    "    # tokenize the text\n",
    "    tokenized_example = tokenizer(\n",
    "        example[\"text\"],\n",
    "        truncation=True,\n",
    "        padding=True,\n",
    "        max_length=sequence_length,\n",
    "    )\n",
    "    return {\"input_ids\": tokenized_example[\"input_ids\"]}\n",
    "\n",
    "# tokenize entire dataset\n",
    "tokenized_datasets = raw_datasets.map(tokenize_function, batched=True, remove_columns=raw_datasets[\"train\"].column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data collator is used to batch the samples together\n",
    "# data pump for training\n",
    "# collate means to collect and combine\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2LMHeadModel(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(5000, 512)\n",
       "    (wpe): Embedding(256, 512)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-5): 6 x GPT2Block(\n",
       "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=512, out_features=5000, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_config = GPT2Config(\n",
    "    vocab_size=tokenizer.vocab_size, # the size of the vocabulary\n",
    "    pad_token_id=tokenizer.pad_token_id, # the token id for padding\n",
    "    n_ctx=sequence_length, # context length\n",
    "    n_positions=sequence_length, # positions in context, the order of the tokens in sequence\n",
    "    n_embd=512, # embedding dimension\n",
    "    n_head=8, # number of heads in the multi-head attention models\n",
    "    n_layer=6, # number of layers\n",
    ")\n",
    "\n",
    "model = GPT2LMHeadModel(model_config)\n",
    "model\n",
    "\n",
    "# wte: word token embeddings; which means the embeddings of the tokens\n",
    "# wpe: word position embeddings; which means the embeddings of the positions of the tokens\n",
    "# drop: dropout\n",
    "# sequence length: 256\n",
    "# dropout layer: 0.1\n",
    "# dropout is a regularization technique; it prevents overfitting\n",
    "# normalisation is done first, which differs to transformers\n",
    "# normalisation means to scale the input to have a mean of 0 and a standard deviation of 1\n",
    "# Conv1D means 1D convolution; convolutions are used to extract features from the input\n",
    "# LayerNorm means layer normalisation; normalisation is used to improve the training of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f9b17e7d08cf4cf5a552d2dbd5326f74",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/66660 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a PreTrainedTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 6.6431, 'learning_rate': 4.962496249624963e-05, 'epoch': 0.08}\n",
      "{'loss': 6.0825, 'learning_rate': 4.924992499249925e-05, 'epoch': 0.15}\n",
      "{'loss': 5.8659, 'learning_rate': 4.887488748874888e-05, 'epoch': 0.23}\n",
      "{'loss': 5.6698, 'learning_rate': 4.84998499849985e-05, 'epoch': 0.3}\n",
      "{'loss': 5.5196, 'learning_rate': 4.812481248124813e-05, 'epoch': 0.38}\n",
      "{'loss': 5.3838, 'learning_rate': 4.774977497749775e-05, 'epoch': 0.45}\n",
      "{'loss': 5.2824, 'learning_rate': 4.737473747374738e-05, 'epoch': 0.53}\n",
      "{'loss': 5.1815, 'learning_rate': 4.6999699969997e-05, 'epoch': 0.6}\n",
      "{'loss': 5.079, 'learning_rate': 4.6624662466246627e-05, 'epoch': 0.68}\n",
      "{'loss': 5.019, 'learning_rate': 4.6249624962496254e-05, 'epoch': 0.75}\n",
      "{'loss': 4.9651, 'learning_rate': 4.5874587458745876e-05, 'epoch': 0.83}\n",
      "{'loss': 4.9066, 'learning_rate': 4.5499549954995503e-05, 'epoch': 0.9}\n",
      "{'loss': 4.861, 'learning_rate': 4.5124512451245125e-05, 'epoch': 0.98}\n",
      "{'loss': 4.7736, 'learning_rate': 4.474947494749475e-05, 'epoch': 1.05}\n",
      "{'loss': 4.7418, 'learning_rate': 4.4374437443744374e-05, 'epoch': 1.13}\n",
      "{'loss': 4.7121, 'learning_rate': 4.3999399939994e-05, 'epoch': 1.2}\n",
      "{'loss': 4.6721, 'learning_rate': 4.362436243624363e-05, 'epoch': 1.28}\n",
      "{'loss': 4.6191, 'learning_rate': 4.324932493249325e-05, 'epoch': 1.35}\n",
      "{'loss': 4.5954, 'learning_rate': 4.287428742874288e-05, 'epoch': 1.43}\n",
      "{'loss': 4.5644, 'learning_rate': 4.24992499249925e-05, 'epoch': 1.5}\n",
      "{'loss': 4.5304, 'learning_rate': 4.212421242124213e-05, 'epoch': 1.58}\n",
      "{'loss': 4.5178, 'learning_rate': 4.174917491749175e-05, 'epoch': 1.65}\n",
      "{'loss': 4.4666, 'learning_rate': 4.137413741374138e-05, 'epoch': 1.73}\n",
      "{'loss': 4.4402, 'learning_rate': 4.0999099909991e-05, 'epoch': 1.8}\n",
      "{'loss': 4.4623, 'learning_rate': 4.0624062406240626e-05, 'epoch': 1.88}\n",
      "{'loss': 4.433, 'learning_rate': 4.0249024902490254e-05, 'epoch': 1.95}\n",
      "{'loss': 4.3943, 'learning_rate': 3.9873987398739875e-05, 'epoch': 2.03}\n",
      "{'loss': 4.3408, 'learning_rate': 3.94989498949895e-05, 'epoch': 2.1}\n",
      "{'loss': 4.3255, 'learning_rate': 3.9123912391239124e-05, 'epoch': 2.18}\n",
      "{'loss': 4.3162, 'learning_rate': 3.874887488748875e-05, 'epoch': 2.25}\n",
      "{'loss': 4.2987, 'learning_rate': 3.837383738373837e-05, 'epoch': 2.33}\n",
      "{'loss': 4.2951, 'learning_rate': 3.7998799879988e-05, 'epoch': 2.4}\n",
      "{'loss': 4.2509, 'learning_rate': 3.762376237623763e-05, 'epoch': 2.48}\n",
      "{'loss': 4.2431, 'learning_rate': 3.724872487248725e-05, 'epoch': 2.55}\n",
      "{'loss': 4.2491, 'learning_rate': 3.687368736873688e-05, 'epoch': 2.63}\n",
      "{'loss': 4.2261, 'learning_rate': 3.64986498649865e-05, 'epoch': 2.7}\n",
      "{'loss': 4.2138, 'learning_rate': 3.612361236123613e-05, 'epoch': 2.78}\n",
      "{'loss': 4.208, 'learning_rate': 3.574857485748575e-05, 'epoch': 2.85}\n",
      "{'loss': 4.1755, 'learning_rate': 3.5373537353735376e-05, 'epoch': 2.93}\n",
      "{'loss': 4.1759, 'learning_rate': 3.4998499849985e-05, 'epoch': 3.0}\n",
      "{'loss': 4.1033, 'learning_rate': 3.4623462346234625e-05, 'epoch': 3.08}\n",
      "{'loss': 4.0995, 'learning_rate': 3.424842484248425e-05, 'epoch': 3.15}\n",
      "{'loss': 4.1105, 'learning_rate': 3.3873387338733874e-05, 'epoch': 3.23}\n",
      "{'loss': 4.093, 'learning_rate': 3.34983498349835e-05, 'epoch': 3.3}\n",
      "{'loss': 4.0617, 'learning_rate': 3.312331233123312e-05, 'epoch': 3.38}\n",
      "{'loss': 4.0852, 'learning_rate': 3.274827482748275e-05, 'epoch': 3.45}\n",
      "{'loss': 4.0563, 'learning_rate': 3.237323732373237e-05, 'epoch': 3.53}\n",
      "{'loss': 4.0799, 'learning_rate': 3.1998199819982e-05, 'epoch': 3.6}\n",
      "{'loss': 4.0499, 'learning_rate': 3.162316231623163e-05, 'epoch': 3.68}\n",
      "{'loss': 4.0728, 'learning_rate': 3.124812481248125e-05, 'epoch': 3.75}\n",
      "{'loss': 4.0506, 'learning_rate': 3.087308730873088e-05, 'epoch': 3.83}\n",
      "{'loss': 4.031, 'learning_rate': 3.0498049804980498e-05, 'epoch': 3.9}\n",
      "{'loss': 4.0282, 'learning_rate': 3.0123012301230126e-05, 'epoch': 3.98}\n",
      "{'loss': 3.9839, 'learning_rate': 2.974797479747975e-05, 'epoch': 4.05}\n",
      "{'loss': 3.9416, 'learning_rate': 2.9372937293729375e-05, 'epoch': 4.13}\n",
      "{'loss': 3.9453, 'learning_rate': 2.8997899789979e-05, 'epoch': 4.2}\n",
      "{'loss': 3.9495, 'learning_rate': 2.8622862286228624e-05, 'epoch': 4.28}\n",
      "{'loss': 3.9128, 'learning_rate': 2.824782478247825e-05, 'epoch': 4.35}\n",
      "{'loss': 3.9432, 'learning_rate': 2.7872787278727873e-05, 'epoch': 4.43}\n",
      "{'loss': 3.9526, 'learning_rate': 2.7497749774977498e-05, 'epoch': 4.5}\n",
      "{'loss': 3.9378, 'learning_rate': 2.7122712271227126e-05, 'epoch': 4.58}\n",
      "{'loss': 3.9458, 'learning_rate': 2.674767476747675e-05, 'epoch': 4.65}\n",
      "{'loss': 3.9138, 'learning_rate': 2.6372637263726375e-05, 'epoch': 4.73}\n",
      "{'loss': 3.9172, 'learning_rate': 2.5997599759976e-05, 'epoch': 4.8}\n",
      "{'loss': 3.9482, 'learning_rate': 2.5622562256225624e-05, 'epoch': 4.88}\n",
      "{'loss': 3.9268, 'learning_rate': 2.5247524752475248e-05, 'epoch': 4.95}\n",
      "{'loss': 3.9126, 'learning_rate': 2.4872487248724873e-05, 'epoch': 5.03}\n",
      "{'loss': 3.8428, 'learning_rate': 2.4497449744974497e-05, 'epoch': 5.1}\n",
      "{'loss': 3.8392, 'learning_rate': 2.4122412241224125e-05, 'epoch': 5.18}\n",
      "{'loss': 3.8347, 'learning_rate': 2.374737473747375e-05, 'epoch': 5.25}\n",
      "{'loss': 3.8446, 'learning_rate': 2.3372337233723374e-05, 'epoch': 5.33}\n",
      "{'loss': 3.8352, 'learning_rate': 2.2997299729973e-05, 'epoch': 5.4}\n",
      "{'loss': 3.838, 'learning_rate': 2.2622262226222623e-05, 'epoch': 5.48}\n",
      "{'loss': 3.8374, 'learning_rate': 2.2247224722472248e-05, 'epoch': 5.55}\n",
      "{'loss': 3.8396, 'learning_rate': 2.1872187218721872e-05, 'epoch': 5.63}\n",
      "{'loss': 3.8185, 'learning_rate': 2.1497149714971497e-05, 'epoch': 5.7}\n",
      "{'loss': 3.819, 'learning_rate': 2.1122112211221125e-05, 'epoch': 5.78}\n",
      "{'loss': 3.8277, 'learning_rate': 2.074707470747075e-05, 'epoch': 5.85}\n",
      "{'loss': 3.8257, 'learning_rate': 2.0372037203720374e-05, 'epoch': 5.93}\n",
      "{'loss': 3.8207, 'learning_rate': 1.999699969997e-05, 'epoch': 6.0}\n",
      "{'loss': 3.747, 'learning_rate': 1.9621962196219623e-05, 'epoch': 6.08}\n",
      "{'loss': 3.7492, 'learning_rate': 1.9246924692469247e-05, 'epoch': 6.15}\n",
      "{'loss': 3.7566, 'learning_rate': 1.8871887188718872e-05, 'epoch': 6.23}\n",
      "{'loss': 3.7584, 'learning_rate': 1.8496849684968497e-05, 'epoch': 6.3}\n",
      "{'loss': 3.7704, 'learning_rate': 1.8121812181218124e-05, 'epoch': 6.38}\n",
      "{'loss': 3.7656, 'learning_rate': 1.774677467746775e-05, 'epoch': 6.45}\n",
      "{'loss': 3.7523, 'learning_rate': 1.7371737173717373e-05, 'epoch': 6.53}\n",
      "{'loss': 3.7448, 'learning_rate': 1.6996699669966998e-05, 'epoch': 6.6}\n",
      "{'loss': 3.7351, 'learning_rate': 1.6621662166216623e-05, 'epoch': 6.68}\n",
      "{'loss': 3.7699, 'learning_rate': 1.6246624662466247e-05, 'epoch': 6.75}\n",
      "{'loss': 3.7392, 'learning_rate': 1.587158715871587e-05, 'epoch': 6.83}\n",
      "{'loss': 3.7578, 'learning_rate': 1.5496549654965496e-05, 'epoch': 6.9}\n",
      "{'loss': 3.7437, 'learning_rate': 1.5121512151215122e-05, 'epoch': 6.98}\n",
      "{'loss': 3.7078, 'learning_rate': 1.4746474647464747e-05, 'epoch': 7.05}\n",
      "{'loss': 3.6885, 'learning_rate': 1.4371437143714373e-05, 'epoch': 7.13}\n",
      "{'loss': 3.6792, 'learning_rate': 1.3996399639963998e-05, 'epoch': 7.2}\n",
      "{'loss': 3.6873, 'learning_rate': 1.3621362136213622e-05, 'epoch': 7.28}\n",
      "{'loss': 3.6844, 'learning_rate': 1.3246324632463247e-05, 'epoch': 7.35}\n",
      "{'loss': 3.7235, 'learning_rate': 1.2871287128712873e-05, 'epoch': 7.43}\n",
      "{'loss': 3.6963, 'learning_rate': 1.2496249624962497e-05, 'epoch': 7.5}\n",
      "{'loss': 3.6834, 'learning_rate': 1.2121212121212122e-05, 'epoch': 7.58}\n",
      "{'loss': 3.7021, 'learning_rate': 1.1746174617461746e-05, 'epoch': 7.65}\n",
      "{'loss': 3.6736, 'learning_rate': 1.1371137113711373e-05, 'epoch': 7.73}\n",
      "{'loss': 3.6779, 'learning_rate': 1.0996099609960997e-05, 'epoch': 7.8}\n",
      "{'loss': 3.7052, 'learning_rate': 1.0621062106210622e-05, 'epoch': 7.88}\n",
      "{'loss': 3.6558, 'learning_rate': 1.0246024602460246e-05, 'epoch': 7.95}\n",
      "{'loss': 3.657, 'learning_rate': 9.870987098709872e-06, 'epoch': 8.03}\n",
      "{'loss': 3.631, 'learning_rate': 9.495949594959497e-06, 'epoch': 8.1}\n",
      "{'loss': 3.6245, 'learning_rate': 9.120912091209122e-06, 'epoch': 8.18}\n",
      "{'loss': 3.6266, 'learning_rate': 8.745874587458746e-06, 'epoch': 8.25}\n",
      "{'loss': 3.6565, 'learning_rate': 8.370837083708372e-06, 'epoch': 8.33}\n",
      "{'loss': 3.649, 'learning_rate': 7.995799579957997e-06, 'epoch': 8.4}\n",
      "{'loss': 3.6512, 'learning_rate': 7.620762076207621e-06, 'epoch': 8.48}\n",
      "{'loss': 3.6414, 'learning_rate': 7.245724572457247e-06, 'epoch': 8.55}\n",
      "{'loss': 3.6422, 'learning_rate': 6.870687068706871e-06, 'epoch': 8.63}\n",
      "{'loss': 3.6507, 'learning_rate': 6.495649564956497e-06, 'epoch': 8.7}\n",
      "{'loss': 3.6318, 'learning_rate': 6.12061206120612e-06, 'epoch': 8.78}\n",
      "{'loss': 3.6283, 'learning_rate': 5.745574557455746e-06, 'epoch': 8.85}\n",
      "{'loss': 3.6362, 'learning_rate': 5.37053705370537e-06, 'epoch': 8.93}\n",
      "{'loss': 3.6299, 'learning_rate': 4.995499549954996e-06, 'epoch': 9.0}\n",
      "{'loss': 3.5864, 'learning_rate': 4.62046204620462e-06, 'epoch': 9.08}\n",
      "{'loss': 3.6028, 'learning_rate': 4.2454245424542455e-06, 'epoch': 9.15}\n",
      "{'loss': 3.5835, 'learning_rate': 3.87038703870387e-06, 'epoch': 9.23}\n",
      "{'loss': 3.5991, 'learning_rate': 3.4953495349534954e-06, 'epoch': 9.3}\n",
      "{'loss': 3.6146, 'learning_rate': 3.1203120312031203e-06, 'epoch': 9.38}\n",
      "{'loss': 3.5994, 'learning_rate': 2.7452745274527453e-06, 'epoch': 9.45}\n",
      "{'loss': 3.6088, 'learning_rate': 2.3702370237023702e-06, 'epoch': 9.53}\n",
      "{'loss': 3.6022, 'learning_rate': 1.995199519951995e-06, 'epoch': 9.6}\n",
      "{'loss': 3.5979, 'learning_rate': 1.6201620162016203e-06, 'epoch': 9.68}\n",
      "{'loss': 3.6003, 'learning_rate': 1.2451245124512453e-06, 'epoch': 9.75}\n",
      "{'loss': 3.6047, 'learning_rate': 8.7008700870087e-07, 'epoch': 9.83}\n",
      "{'loss': 3.5767, 'learning_rate': 4.950495049504951e-07, 'epoch': 9.9}\n",
      "{'loss': 3.6094, 'learning_rate': 1.2001200120012004e-07, 'epoch': 9.98}\n",
      "{'train_runtime': 36625.4247, 'train_samples_per_second': 94.641, 'train_steps_per_second': 1.82, 'train_loss': 4.0746862393758905, 'epoch': 10.0}\n"
     ]
    }
   ],
   "source": [
    "# clear VRAM GPU cache\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "output_path = \"output\"\n",
    "\n",
    "# Create the Trainer\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=output_path, # output directory\n",
    "    overwrite_output_dir=True, # overwrite the content of the output directory\n",
    "    num_train_epochs=10, # number of training epochs\n",
    "    #per_device_train_batch_size=16, # batch size for training per device (e.g. multiple GPUs), which took 8 minutes\n",
    "    #per_device_train_batch_size=32 # double, because i was only using around 3GB of VRAM, which took 7.5 minutes\n",
    "    per_device_train_batch_size=52 # increase, because i was only using around 6GB of VRAM with 32, which also took 7.5 minutes\n",
    "    # now uses 7704MiB of VRAM\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model, # the model\n",
    "    args=training_args, # training arguments\n",
    "    data_collator=data_collator, # data collator\n",
    "    train_dataset=tokenized_datasets[\"train\"] # training dataset\n",
    ")\n",
    "\n",
    "# Train\n",
    "trainer.train()\n",
    "\n",
    "# Save\n",
    "tokenizer.save_pretrained(output_path)\n",
    "model.save_pretrained(output_path)\n",
    "\n",
    "# TODO:\n",
    "# You're using a PreTrainedTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1896,  363,   91]], device='cuda:0')\n",
      "Jhana meditation is not a great deal of thinking and thinking, but it is very difficult to understand.................................................................................\n"
     ]
    }
   ],
   "source": [
    "# Encode the conditioning tokens.\n",
    "input_ids = tokenizer.encode(\"Jhana meditation is \", return_tensors=\"pt\").cuda()\n",
    "print(input_ids)\n",
    "\n",
    "# Generate more tokens.\n",
    "generated_ids = model.generate(\n",
    "    input_ids,\n",
    "    max_length=100,\n",
    "    do_sample=True,\n",
    "    temperature=0.5\n",
    ")\n",
    "generated_sequence = tokenizer.decode(generated_ids[0], clean_up_tokenization_spaces=True)\n",
    "print(generated_sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1896,  363,   91]], device='cuda:0')\n",
      "Jhana meditation is a very important thing to do...........................................................................................\n"
     ]
    }
   ],
   "source": [
    "# Assuming 'tokenizer' and 'model' are already defined\n",
    "\n",
    "# Move model to GPU\n",
    "model = model.cuda()\n",
    "\n",
    "# Encode the conditioning tokens and move them to GPU\n",
    "input_ids = tokenizer.encode(\"Jhana meditation is \", return_tensors=\"pt\").cuda()\n",
    "print(input_ids)\n",
    "\n",
    "# Generate more tokens\n",
    "generated_ids = model.generate(\n",
    "    input_ids,\n",
    "    max_length=100,\n",
    "    do_sample=True,\n",
    "    temperature=0.5\n",
    ")\n",
    "\n",
    "# Decode generated ids\n",
    "generated_sequence = tokenizer.decode(generated_ids[0], clean_up_tokenization_spaces=True)\n",
    "print(generated_sequence)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2490,  292,  456,  363,    5,  152,  481]], device='cuda:0')\n",
      "To practice jhana meditation, we sit down to meditate and we can use the meditation as a meditation.................................................................................\n"
     ]
    }
   ],
   "source": [
    "# Encode the conditioning tokens.\n",
    "input_ids = tokenizer.encode(\"To practice jhana meditation, we sit \", return_tensors=\"pt\").cuda()\n",
    "print(input_ids)\n",
    "\n",
    "# Generate more tokens.\n",
    "generated_ids = model.generate(\n",
    "    input_ids,\n",
    "    max_length=100,\n",
    "    do_sample=True,\n",
    "    temperature=0.5\n",
    ")\n",
    "generated_sequence = tokenizer.decode(generated_ids[0], clean_up_tokenization_spaces=True)\n",
    "print(generated_sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input text: Jhana meditation is \n",
      "Encoded input_ids: tensor([[1896,  363,   91]], device='cuda:0') on device cuda:0\n",
      "Raw generated ids: tensor([[1896,  363,   91,  530,   90,  148,    6, 1286,  771,    7,    7,    7,\n",
      "            7,    7,    7,    7,    7,    7,    7,    7,    7,    7,    7,    7,\n",
      "            7,    7,    7,    7,    7,    7,    7,    7,    7,    7,    7,    7,\n",
      "            7,    7,    7,    7,    7,    7,    7,    7,    7,    7,    7,    7,\n",
      "            7,    7,    7,    7,    7,    7,    7,    7,    7,    7,    7,    7,\n",
      "            7,    7,    7,    7,    7,    7,    7,    7,    7,    7,    7,    7,\n",
      "            7,    7,    7,    7,    7,  679,    7,    7,    7,    7,    7,    7,\n",
      "            7,    7,    7,    7,    7,    7,    7,    7,    7,    7,    7,    7,\n",
      "          679,    7,    7,  679]], device='cuda:0')\n",
      "Decoded generated sequence: Jhana meditation is called the mind - door process.................................................................... ().................. ().. ()\n"
     ]
    }
   ],
   "source": [
    "# Assuming 'tokenizer' and 'model' are already defined\n",
    "\n",
    "# Move model to GPU\n",
    "model = model.cuda()\n",
    "\n",
    "# Encode the conditioning tokens and move them to GPU\n",
    "input_text = \"Jhana meditation is \"\n",
    "input_ids = tokenizer.encode(input_text, return_tensors=\"pt\").cuda()\n",
    "print(f\"Input text: {input_text}\")\n",
    "print(f\"Encoded input_ids: {input_ids} on device {input_ids.device}\")\n",
    "\n",
    "# Generate more tokens\n",
    "generated_ids = model.generate(\n",
    "    input_ids,\n",
    "    max_length=100,\n",
    "    do_sample=True,\n",
    "    temperature=0.5\n",
    ")\n",
    "print(f\"Raw generated ids: {generated_ids}\")\n",
    "\n",
    "# Decode generated ids\n",
    "generated_sequence = tokenizer.decode(generated_ids[0], clean_up_tokenization_spaces=True)\n",
    "print(f\"Decoded generated sequence: {generated_sequence}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input text: 'Jhana meditation is '\n"
     ]
    }
   ],
   "source": [
    "input_text = \"Jhana meditation is \"\n",
    "print(f\"Input text: '{input_text}'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens: ['Jhana', 'meditation', 'is']\n"
     ]
    }
   ],
   "source": [
    "tokens = tokenizer.tokenize(input_text)\n",
    "print(f\"Tokens: {tokens}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded input_ids: tensor([[1896,  363,   91]])\n",
      "Decoded tokens from input_ids: ['Jhana', 'meditation', 'is']\n"
     ]
    }
   ],
   "source": [
    "input_ids = tokenizer.encode(input_text, return_tensors=\"pt\")\n",
    "print(f\"Encoded input_ids: {input_ids}\")\n",
    "decoded_tokens = [tokenizer.decode([id]) for id in input_ids[0]]\n",
    "print(f\"Decoded tokens from input_ids: {decoded_tokens}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tokens: 3\n",
      "Number of input_ids: 3\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of tokens: {len(tokens)}\")\n",
    "print(f\"Number of input_ids: {input_ids.shape[1]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Special tokens added: ['Jhana', 'meditation', 'is']\n"
     ]
    }
   ],
   "source": [
    "print(f\"Special tokens added: {tokenizer.build_inputs_with_special_tokens(tokens)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention mask: tensor([[1, 1, 1]])\n"
     ]
    }
   ],
   "source": [
    "attention_mask = tokenizer(input_text, return_tensors=\"pt\")[\"attention_mask\"]\n",
    "print(f\"Attention mask: {attention_mask}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model device: cuda:0\n",
      "Input ids device: cpu\n"
     ]
    }
   ],
   "source": [
    "print(f\"Model device: {next(model.parameters()).device}\")\n",
    "print(f\"Input ids device: {input_ids.device}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jhana meditation is a very interesting thing, very interesting thing.................................................................. ()..................... ()\n"
     ]
    }
   ],
   "source": [
    "# Ensure input_ids are on the same device as the model\n",
    "input_ids = input_ids.to(next(model.parameters()).device)\n",
    "\n",
    "# Now you can proceed with generating more tokens\n",
    "generated_ids = model.generate(\n",
    "    input_ids,\n",
    "    max_length=100,\n",
    "    do_sample=True,\n",
    "    temperature=0.5\n",
    ")\n",
    "\n",
    "# Decode generated ids\n",
    "generated_sequence = tokenizer.decode(generated_ids[0], clean_up_tokenization_spaces=True)\n",
    "print(generated_sequence)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model device: cuda:0\n",
      "Input ids device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "print(f\"Model device: {next(model.parameters()).device}\")\n",
    "print(f\"Input ids device: {input_ids.device}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model locally\n",
    "\n",
    "#model = GPT2LMHeadModel.from_pretrained(pretrained_model_name_or_path=path)\n",
    "#tokenizer = PreTrainedTokenizerFast(pretrained_model_name_or_path=path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jhana_ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
