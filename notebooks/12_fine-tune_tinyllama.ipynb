{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Guide to Fine-Tuning TinyLlama on `jhana-question-answer` Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/OpenAccess-AI-Collective/axolotl/tree/main\n",
    "\n",
    "%cd axolotl\n",
    "\n",
    "# follow instructions in the README to install the dependencies: https://github.com/OpenAccess-AI-Collective/axolotl/tree/main\n",
    "\n",
    "# create a new conda environment\n",
    "!conda create -n jhana-axolotl python=3.10\n",
    "\n",
    "pip3 install packaging\n",
    "\n",
    "pip3 install -e '.[flash-attn,deepspeed]'\n",
    "\n",
    "# i had some trouble with dependencies; these packages worked for me\n",
    "\n",
    "\"\"\"\n",
    "accelerate==0.26.1\n",
    "addict==2.4.0\n",
    "aiobotocore==2.12.0\n",
    "art==6.1\n",
    "bitsandbytes==0.42.0\n",
    "bitsandbytes_cuda112==0.26.0.post2\n",
    "colorama==0.4.6\n",
    "deepspeed==0.13.1\n",
    "einops==0.7.0\n",
    "evaluate==0.4.1\n",
    "fire==0.5.0\n",
    "flash_attn==2.5.0\n",
    "fschat==0.2.36\n",
    "gcsfs==2024.2.0\n",
    "gradio==4.19.2\n",
    "huggingface_hub==0.21.3\n",
    "Jinja2==3.1.3\n",
    "lion_pytorch==0.1.2\n",
    "mamba_ssm==1.2.0.post1\n",
    "mlflow==2.11.0\n",
    "modal==0.61.3\n",
    "numba==0.59.0\n",
    "numpy==1.26.4\n",
    "optimum==1.16.2\n",
    "pandas==2.2.1\n",
    "peft==0.9.1.dev0\n",
    "pydantic==2.6.3\n",
    "pynvml==11.5.0\n",
    "pytest==8.1.0\n",
    "PyYAML==6.0.1\n",
    "PyYAML==6.0.1\n",
    "Requests==2.31.0\n",
    "s3fs==2024.2.0\n",
    "safetensors==0.4.2\n",
    "setuptools==69.1.1\n",
    "termcolor==2.4.0\n",
    "tokenizers==0.15.2\n",
    "torch==2.1.1\n",
    "tqdm==4.66.2\n",
    "transformers==4.39.0.dev0\n",
    "wandb==0.16.3\n",
    "xformers==0.0.23\n",
    "\"\"\"\n",
    "\n",
    "# modify the tiny-llama/lora.yml file\n",
    "\n",
    "\"\"\"\n",
    "base_model: TinyLlama/TinyLlama-1.1B-intermediate-step-1431k-3T\n",
    "model_type: LlamaForCausalLM\n",
    "tokenizer_type: LlamaTokenizer\n",
    "\n",
    "load_in_8bit: true\n",
    "load_in_4bit: false\n",
    "strict: false\n",
    "\n",
    "datasets:\n",
    "  #- path: mhenrichsen/alpaca_2k_test\n",
    "  - path: carecodeconnect/jhana-question-answer\n",
    "    type: alpaca\n",
    "dataset_prepared_path:\n",
    "val_set_size: 0.05\n",
    "output_dir: ./jhana-tinyllama\n",
    "\n",
    "sequence_len: 4096\n",
    "sample_packing: false # was true\n",
    "pad_to_sequence_len: true\n",
    "\n",
    "adapter: lora\n",
    "lora_model_dir:\n",
    "lora_r: 32\n",
    "lora_alpha: 16\n",
    "lora_dropout: 0.05\n",
    "lora_target_linear: true\n",
    "lora_fan_in_fan_out:\n",
    "\n",
    "wandb_project:\n",
    "wandb_entity:\n",
    "wandb_watch:\n",
    "wandb_name:\n",
    "wandb_log_model:\n",
    "\n",
    "gradient_accumulation_steps: 4 # could increase to 8\n",
    "micro_batch_size: 2 # could decrease to 1\n",
    "num_epochs: 4\n",
    "optimizer: adamw_bnb_8bit\n",
    "lr_scheduler: cosine\n",
    "learning_rate: 0.0002\n",
    "\n",
    "train_on_inputs: false\n",
    "group_by_length: false\n",
    "bf16: auto\n",
    "fp16:\n",
    "tf32: false\n",
    "\n",
    "gradient_checkpointing: true\n",
    "early_stopping_patience:\n",
    "resume_from_checkpoint:\n",
    "local_rank:\n",
    "logging_steps: 1\n",
    "xformers_attention:\n",
    "flash_attention: false\n",
    "\n",
    "warmup_steps: 10\n",
    "evals_per_epoch: 4\n",
    "saves_per_epoch: 1\n",
    "debug:\n",
    "deepspeed:\n",
    "weight_decay: 0.0\n",
    "fsdp:\n",
    "fsdp_config:\n",
    "special_tokens:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess data\n",
    "!CUDA_VISIBLE_DEVICES=0 python -m axolotl.cli.preprocess examples/tiny-llama/lora.yml\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train tiny-llama on the Jhana dataset\n",
    "!accelerate launch -m axolotl.cli.train examples/tiny-llama/lora.yml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run inference using axolotl terminal tool\n",
    "!python -m axolotl.cli.inference examples/tiny-llama/lora.yml --lora_model_dir=\"./jhana-tinyllama\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jhana-tinyllama",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
