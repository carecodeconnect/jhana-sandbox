{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow\n",
      "  Using cached tensorflow-2.15.0.post1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.2 kB)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /home/solaris/miniconda3/envs/jhana_ai_gpt2/lib/python3.10/site-packages (from tensorflow) (2.1.0)\n",
      "Collecting astunparse>=1.6.0 (from tensorflow)\n",
      "  Downloading astunparse-1.6.3-py2.py3-none-any.whl.metadata (4.4 kB)\n",
      "Collecting flatbuffers>=23.5.26 (from tensorflow)\n",
      "  Using cached flatbuffers-23.5.26-py2.py3-none-any.whl.metadata (850 bytes)\n",
      "Collecting gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 (from tensorflow)\n",
      "  Using cached gast-0.5.4-py3-none-any.whl.metadata (1.3 kB)\n",
      "Collecting google-pasta>=0.1.1 (from tensorflow)\n",
      "  Downloading google_pasta-0.2.0-py3-none-any.whl.metadata (814 bytes)\n",
      "Collecting h5py>=2.9.0 (from tensorflow)\n",
      "  Using cached h5py-3.10.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.5 kB)\n",
      "Collecting libclang>=13.0.0 (from tensorflow)\n",
      "  Using cached libclang-16.0.6-py2.py3-none-manylinux2010_x86_64.whl.metadata (5.2 kB)\n",
      "Collecting ml-dtypes~=0.2.0 (from tensorflow)\n",
      "  Using cached ml_dtypes-0.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (20 kB)\n",
      "Collecting numpy<2.0.0,>=1.23.5 (from tensorflow)\n",
      "  Using cached numpy-1.26.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
      "Collecting opt-einsum>=2.3.2 (from tensorflow)\n",
      "  Downloading opt_einsum-3.3.0-py3-none-any.whl.metadata (6.5 kB)\n",
      "Requirement already satisfied: packaging in /home/solaris/miniconda3/envs/jhana_ai_gpt2/lib/python3.10/site-packages (from tensorflow) (23.2)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /home/solaris/miniconda3/envs/jhana_ai_gpt2/lib/python3.10/site-packages (from tensorflow) (4.25.3)\n",
      "Requirement already satisfied: setuptools in /home/solaris/miniconda3/envs/jhana_ai_gpt2/lib/python3.10/site-packages (from tensorflow) (69.1.0)\n",
      "Requirement already satisfied: six>=1.12.0 in /home/solaris/miniconda3/envs/jhana_ai_gpt2/lib/python3.10/site-packages (from tensorflow) (1.16.0)\n",
      "Collecting termcolor>=1.1.0 (from tensorflow)\n",
      "  Using cached termcolor-2.4.0-py3-none-any.whl.metadata (6.1 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /home/solaris/miniconda3/envs/jhana_ai_gpt2/lib/python3.10/site-packages (from tensorflow) (4.10.0)\n",
      "Collecting wrapt<1.15,>=1.11.0 (from tensorflow)\n",
      "  Downloading wrapt-1.14.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
      "Collecting tensorflow-io-gcs-filesystem>=0.23.1 (from tensorflow)\n",
      "  Using cached tensorflow_io_gcs_filesystem-0.36.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (14 kB)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /home/solaris/miniconda3/envs/jhana_ai_gpt2/lib/python3.10/site-packages (from tensorflow) (1.60.1)\n",
      "Collecting tensorboard<2.16,>=2.15 (from tensorflow)\n",
      "  Using cached tensorboard-2.15.2-py3-none-any.whl.metadata (1.7 kB)\n",
      "Collecting tensorflow-estimator<2.16,>=2.15.0 (from tensorflow)\n",
      "  Using cached tensorflow_estimator-2.15.0-py2.py3-none-any.whl.metadata (1.3 kB)\n",
      "Collecting keras<2.16,>=2.15.0 (from tensorflow)\n",
      "  Using cached keras-2.15.0-py3-none-any.whl.metadata (2.4 kB)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /home/solaris/miniconda3/envs/jhana_ai_gpt2/lib/python3.10/site-packages (from astunparse>=1.6.0->tensorflow) (0.42.0)\n",
      "Collecting google-auth<3,>=1.6.3 (from tensorboard<2.16,>=2.15->tensorflow)\n",
      "  Downloading google_auth-2.28.1-py2.py3-none-any.whl.metadata (4.7 kB)\n",
      "Collecting google-auth-oauthlib<2,>=0.5 (from tensorboard<2.16,>=2.15->tensorflow)\n",
      "  Using cached google_auth_oauthlib-1.2.0-py2.py3-none-any.whl.metadata (2.7 kB)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /home/solaris/miniconda3/envs/jhana_ai_gpt2/lib/python3.10/site-packages (from tensorboard<2.16,>=2.15->tensorflow) (3.5.2)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /home/solaris/miniconda3/envs/jhana_ai_gpt2/lib/python3.10/site-packages (from tensorboard<2.16,>=2.15->tensorflow) (2.31.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /home/solaris/miniconda3/envs/jhana_ai_gpt2/lib/python3.10/site-packages (from tensorboard<2.16,>=2.15->tensorflow) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /home/solaris/miniconda3/envs/jhana_ai_gpt2/lib/python3.10/site-packages (from tensorboard<2.16,>=2.15->tensorflow) (3.0.1)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /home/solaris/miniconda3/envs/jhana_ai_gpt2/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (5.3.2)\n",
      "Collecting pyasn1-modules>=0.2.1 (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow)\n",
      "  Using cached pyasn1_modules-0.3.0-py2.py3-none-any.whl.metadata (3.6 kB)\n",
      "Collecting rsa<5,>=3.1.4 (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow)\n",
      "  Downloading rsa-4.9-py3-none-any.whl.metadata (4.2 kB)\n",
      "Collecting requests-oauthlib>=0.7.0 (from google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow)\n",
      "  Downloading requests_oauthlib-1.3.1-py2.py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/solaris/miniconda3/envs/jhana_ai_gpt2/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/solaris/miniconda3/envs/jhana_ai_gpt2/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/solaris/miniconda3/envs/jhana_ai_gpt2/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/solaris/miniconda3/envs/jhana_ai_gpt2/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (2024.2.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /home/solaris/miniconda3/envs/jhana_ai_gpt2/lib/python3.10/site-packages (from werkzeug>=1.0.1->tensorboard<2.16,>=2.15->tensorflow) (2.1.5)\n",
      "Collecting pyasn1<0.6.0,>=0.4.6 (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow)\n",
      "  Using cached pyasn1-0.5.1-py2.py3-none-any.whl.metadata (8.6 kB)\n",
      "Collecting oauthlib>=3.0.0 (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow)\n",
      "  Downloading oauthlib-3.2.2-py3-none-any.whl.metadata (7.5 kB)\n",
      "Using cached tensorflow-2.15.0.post1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (475.2 MB)\n",
      "Using cached astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Using cached flatbuffers-23.5.26-py2.py3-none-any.whl (26 kB)\n",
      "Using cached gast-0.5.4-py3-none-any.whl (19 kB)\n",
      "Using cached google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "Using cached h5py-3.10.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.8 MB)\n",
      "Using cached keras-2.15.0-py3-none-any.whl (1.7 MB)\n",
      "Using cached libclang-16.0.6-py2.py3-none-manylinux2010_x86_64.whl (22.9 MB)\n",
      "Using cached ml_dtypes-0.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB)\n",
      "Using cached numpy-1.26.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.2 MB)\n",
      "Using cached opt_einsum-3.3.0-py3-none-any.whl (65 kB)\n",
      "Using cached tensorboard-2.15.2-py3-none-any.whl (5.5 MB)\n",
      "Using cached tensorflow_estimator-2.15.0-py2.py3-none-any.whl (441 kB)\n",
      "Using cached tensorflow_io_gcs_filesystem-0.36.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.1 MB)\n",
      "Using cached termcolor-2.4.0-py3-none-any.whl (7.7 kB)\n",
      "Using cached wrapt-1.14.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (77 kB)\n",
      "Downloading google_auth-2.28.1-py2.py3-none-any.whl (186 kB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m186.9/186.9 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m[36m0:00:01\u001b[0m36m0:00:01\u001b[0m\n",
      "\u001b[?25hUsing cached google_auth_oauthlib-1.2.0-py2.py3-none-any.whl (24 kB)\n",
      "Using cached pyasn1_modules-0.3.0-py2.py3-none-any.whl (181 kB)\n",
      "Using cached requests_oauthlib-1.3.1-py2.py3-none-any.whl (23 kB)\n",
      "Using cached rsa-4.9-py3-none-any.whl (34 kB)\n",
      "Using cached oauthlib-3.2.2-py3-none-any.whl (151 kB)\n",
      "Using cached pyasn1-0.5.1-py2.py3-none-any.whl (84 kB)\n",
      "Installing collected packages: libclang, flatbuffers, wrapt, termcolor, tensorflow-io-gcs-filesystem, tensorflow-estimator, pyasn1, oauthlib, numpy, keras, google-pasta, gast, astunparse, rsa, requests-oauthlib, pyasn1-modules, opt-einsum, ml-dtypes, h5py, google-auth, google-auth-oauthlib, tensorboard, tensorflow\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.22.0\n",
      "    Uninstalling numpy-1.22.0:\n",
      "      Successfully uninstalled numpy-1.22.0\n",
      "  Attempting uninstall: tensorboard\n",
      "    Found existing installation: tensorboard 2.16.2\n",
      "    Uninstalling tensorboard-2.16.2:\n",
      "      Successfully uninstalled tensorboard-2.16.2\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "tts 0.22.0 requires numpy==1.22.0; python_version <= \"3.10\", but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed astunparse-1.6.3 flatbuffers-23.5.26 gast-0.5.4 google-auth-2.28.1 google-auth-oauthlib-1.2.0 google-pasta-0.2.0 h5py-3.10.0 keras-2.15.0 libclang-16.0.6 ml-dtypes-0.2.0 numpy-1.26.4 oauthlib-3.2.2 opt-einsum-3.3.0 pyasn1-0.5.1 pyasn1-modules-0.3.0 requests-oauthlib-1.3.1 rsa-4.9 tensorboard-2.15.2 tensorflow-2.15.0.post1 tensorflow-estimator-2.15.0 tensorflow-io-gcs-filesystem-0.36.0 termcolor-2.4.0 wrapt-1.14.1\n",
      "Collecting transformers==4.33.0\n",
      "  Using cached transformers-4.33.0-py3-none-any.whl.metadata (119 kB)\n",
      "Requirement already satisfied: tokenizers in /home/solaris/miniconda3/envs/jhana_ai_gpt2/lib/python3.10/site-packages (0.15.2)\n",
      "Collecting datasets\n",
      "  Downloading datasets-2.17.1-py3-none-any.whl.metadata (20 kB)\n",
      "Collecting accelerate\n",
      "  Using cached accelerate-0.27.2-py3-none-any.whl.metadata (18 kB)\n",
      "Requirement already satisfied: filelock in /home/solaris/miniconda3/envs/jhana_ai_gpt2/lib/python3.10/site-packages (from transformers==4.33.0) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.15.1 in /home/solaris/miniconda3/envs/jhana_ai_gpt2/lib/python3.10/site-packages (from transformers==4.33.0) (0.20.3)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/solaris/miniconda3/envs/jhana_ai_gpt2/lib/python3.10/site-packages (from transformers==4.33.0) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/solaris/miniconda3/envs/jhana_ai_gpt2/lib/python3.10/site-packages (from transformers==4.33.0) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/solaris/miniconda3/envs/jhana_ai_gpt2/lib/python3.10/site-packages (from transformers==4.33.0) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/solaris/miniconda3/envs/jhana_ai_gpt2/lib/python3.10/site-packages (from transformers==4.33.0) (2023.12.25)\n",
      "Requirement already satisfied: requests in /home/solaris/miniconda3/envs/jhana_ai_gpt2/lib/python3.10/site-packages (from transformers==4.33.0) (2.31.0)\n",
      "Collecting tokenizers\n",
      "  Using cached tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /home/solaris/miniconda3/envs/jhana_ai_gpt2/lib/python3.10/site-packages (from transformers==4.33.0) (0.4.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/solaris/miniconda3/envs/jhana_ai_gpt2/lib/python3.10/site-packages (from transformers==4.33.0) (4.66.2)\n",
      "Requirement already satisfied: pyarrow>=12.0.0 in /home/solaris/miniconda3/envs/jhana_ai_gpt2/lib/python3.10/site-packages (from datasets) (15.0.0)\n",
      "Collecting pyarrow-hotfix (from datasets)\n",
      "  Using cached pyarrow_hotfix-0.6-py3-none-any.whl.metadata (3.6 kB)\n",
      "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
      "  Using cached dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: pandas in /home/solaris/miniconda3/envs/jhana_ai_gpt2/lib/python3.10/site-packages (from datasets) (1.5.3)\n",
      "Collecting xxhash (from datasets)\n",
      "  Using cached xxhash-3.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
      "Collecting multiprocess (from datasets)\n",
      "  Using cached multiprocess-0.70.16-py310-none-any.whl.metadata (7.2 kB)\n",
      "Collecting fsspec<=2023.10.0,>=2023.1.0 (from fsspec[http]<=2023.10.0,>=2023.1.0->datasets)\n",
      "  Using cached fsspec-2023.10.0-py3-none-any.whl.metadata (6.8 kB)\n",
      "Requirement already satisfied: aiohttp in /home/solaris/miniconda3/envs/jhana_ai_gpt2/lib/python3.10/site-packages (from datasets) (3.9.3)\n",
      "Requirement already satisfied: psutil in /home/solaris/miniconda3/envs/jhana_ai_gpt2/lib/python3.10/site-packages (from accelerate) (5.9.8)\n",
      "Requirement already satisfied: torch>=1.10.0 in /home/solaris/miniconda3/envs/jhana_ai_gpt2/lib/python3.10/site-packages (from accelerate) (2.2.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/solaris/miniconda3/envs/jhana_ai_gpt2/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/solaris/miniconda3/envs/jhana_ai_gpt2/lib/python3.10/site-packages (from aiohttp->datasets) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/solaris/miniconda3/envs/jhana_ai_gpt2/lib/python3.10/site-packages (from aiohttp->datasets) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/solaris/miniconda3/envs/jhana_ai_gpt2/lib/python3.10/site-packages (from aiohttp->datasets) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /home/solaris/miniconda3/envs/jhana_ai_gpt2/lib/python3.10/site-packages (from aiohttp->datasets) (1.9.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /home/solaris/miniconda3/envs/jhana_ai_gpt2/lib/python3.10/site-packages (from aiohttp->datasets) (4.0.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/solaris/miniconda3/envs/jhana_ai_gpt2/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.15.1->transformers==4.33.0) (4.10.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/solaris/miniconda3/envs/jhana_ai_gpt2/lib/python3.10/site-packages (from requests->transformers==4.33.0) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/solaris/miniconda3/envs/jhana_ai_gpt2/lib/python3.10/site-packages (from requests->transformers==4.33.0) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/solaris/miniconda3/envs/jhana_ai_gpt2/lib/python3.10/site-packages (from requests->transformers==4.33.0) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/solaris/miniconda3/envs/jhana_ai_gpt2/lib/python3.10/site-packages (from requests->transformers==4.33.0) (2024.2.2)\n",
      "Requirement already satisfied: sympy in /home/solaris/miniconda3/envs/jhana_ai_gpt2/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (1.12)\n",
      "Requirement already satisfied: networkx in /home/solaris/miniconda3/envs/jhana_ai_gpt2/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (2.8.8)\n",
      "Requirement already satisfied: jinja2 in /home/solaris/miniconda3/envs/jhana_ai_gpt2/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.1.3)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /home/solaris/miniconda3/envs/jhana_ai_gpt2/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /home/solaris/miniconda3/envs/jhana_ai_gpt2/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /home/solaris/miniconda3/envs/jhana_ai_gpt2/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /home/solaris/miniconda3/envs/jhana_ai_gpt2/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /home/solaris/miniconda3/envs/jhana_ai_gpt2/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /home/solaris/miniconda3/envs/jhana_ai_gpt2/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /home/solaris/miniconda3/envs/jhana_ai_gpt2/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /home/solaris/miniconda3/envs/jhana_ai_gpt2/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /home/solaris/miniconda3/envs/jhana_ai_gpt2/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /home/solaris/miniconda3/envs/jhana_ai_gpt2/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (2.19.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /home/solaris/miniconda3/envs/jhana_ai_gpt2/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
      "Requirement already satisfied: triton==2.2.0 in /home/solaris/miniconda3/envs/jhana_ai_gpt2/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (2.2.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /home/solaris/miniconda3/envs/jhana_ai_gpt2/lib/python3.10/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.10.0->accelerate) (12.3.101)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /home/solaris/miniconda3/envs/jhana_ai_gpt2/lib/python3.10/site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/solaris/miniconda3/envs/jhana_ai_gpt2/lib/python3.10/site-packages (from pandas->datasets) (2024.1)\n",
      "Requirement already satisfied: six>=1.5 in /home/solaris/miniconda3/envs/jhana_ai_gpt2/lib/python3.10/site-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/solaris/miniconda3/envs/jhana_ai_gpt2/lib/python3.10/site-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.5)\n",
      "Requirement already satisfied: mpmath>=0.19 in /home/solaris/miniconda3/envs/jhana_ai_gpt2/lib/python3.10/site-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n",
      "Using cached transformers-4.33.0-py3-none-any.whl (7.6 MB)\n",
      "Using cached tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
      "Downloading datasets-2.17.1-py3-none-any.whl (536 kB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m536.7/536.7 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m\n",
      "\u001b[?25hUsing cached accelerate-0.27.2-py3-none-any.whl (279 kB)\n",
      "Using cached dill-0.3.8-py3-none-any.whl (116 kB)\n",
      "Using cached fsspec-2023.10.0-py3-none-any.whl (166 kB)\n",
      "Using cached multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
      "Using cached pyarrow_hotfix-0.6-py3-none-any.whl (7.9 kB)\n",
      "Using cached xxhash-3.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
      "Installing collected packages: tokenizers, xxhash, pyarrow-hotfix, fsspec, dill, multiprocess, transformers, datasets, accelerate\n",
      "  Attempting uninstall: tokenizers\n",
      "    Found existing installation: tokenizers 0.15.2\n",
      "    Uninstalling tokenizers-0.15.2:\n",
      "      Successfully uninstalled tokenizers-0.15.2\n",
      "  Attempting uninstall: fsspec\n",
      "    Found existing installation: fsspec 2024.2.0\n",
      "    Uninstalling fsspec-2024.2.0:\n",
      "      Successfully uninstalled fsspec-2024.2.0\n",
      "  Attempting uninstall: transformers\n",
      "    Found existing installation: transformers 4.38.1\n",
      "    Uninstalling transformers-4.38.1:\n",
      "      Successfully uninstalled transformers-4.38.1\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "tts 0.22.0 requires numpy==1.22.0; python_version <= \"3.10\", but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed accelerate-0.27.2 datasets-2.17.1 dill-0.3.8 fsspec-2023.10.0 multiprocess-0.70.16 pyarrow-hotfix-0.6 tokenizers-0.13.3 transformers-4.33.0 xxhash-3.4.1\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow  # This will install TensorFlow along with a compatible numpy version\n",
    "!pip install transformers==4.33.0 tokenizers datasets accelerate\n",
    "# Install tts with a version compatible with the numpy version required by TensorFlow, if available\n",
    "# !pip install tts==compatible_version_here\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-29 13:38:07.596521: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-02-29 13:38:07.596543: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-02-29 13:38:07.597297: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-02-29 13:38:07.601584: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-02-29 13:38:08.486756: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "/home/solaris/miniconda3/envs/jhana_ai_gpt2/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/solaris/miniconda3/envs/jhana_ai_gpt2/lib/python3.10/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n",
      "/home/solaris/miniconda3/envs/jhana_ai_gpt2/lib/python3.10/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n",
      "2024-02-29 13:38:16.168403: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-02-29 13:38:16.172858: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-02-29 13:38:16.172971: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import glob\n",
    "import os\n",
    "import shutil\n",
    "import tqdm\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import BPE\n",
    "from tokenizers.trainers import BpeTrainer\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "from transformers import PreTrainedTokenizerFast\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "from transformers import GPT2Config, GPT2LMHeadModel\n",
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "tf.config.list_physical_devices(\"GPU\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import os\n",
    "import re\n",
    "import glob\n",
    "\n",
    "# Define the source and target directories\n",
    "source_dir = '../data/input/clean_data'\n",
    "target_dir = '../data/input/filtered_data'\n",
    "dataset_file = os.path.join(target_dir, \"filtered_dataset.txt\")\n",
    "\n",
    "# Ensure the target directory exists\n",
    "os.makedirs(target_dir, exist_ok=True)\n",
    "\n",
    "# Regex pattern to keep English letters, Pali diacritics, and standard punctuation\n",
    "pattern = re.compile(r\"[^a-zA-ZāīūṅñṭḍṇḷĀĪŪṄÑṬḌṆḶ\\s.,!?()-]\")\n",
    "\n",
    "# Function to clean text by removing unwanted characters\n",
    "def clean_text(text):\n",
    "    return pattern.sub('', text)\n",
    "\n",
    "# Find all .txt files in the source directory\n",
    "paths = glob.glob(os.path.join(source_dir, \"*.txt\"))[:1000]  # Limit to first 1000 files\n",
    "\n",
    "# Process and merge content of each file\n",
    "with open(dataset_file, \"w\", encoding='utf-8') as output_file:\n",
    "    for path in paths:\n",
    "        with open(path, \"r\", encoding='utf-8') as file:\n",
    "            for line in file:\n",
    "                cleaned_line = clean_text(line.strip())\n",
    "                if cleaned_line:\n",
    "                    print(cleaned_line, file=output_file)\n",
    "\n",
    "print(\"Corpus loaded and filtered.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import os\n",
    "import re\n",
    "import glob\n",
    "import shutil\n",
    "\n",
    "# Define the source directories for copying .txt files\n",
    "source_dirs = [\n",
    "    \"../data/input/text/books_converted\",\n",
    "    \"../data/input/text/web_txt\",\n",
    "    \"../data/input/audio_transcribed/transcribed_youtube\",\n",
    "    \"../data/input/audio_transcribed/transscribed_audio_dharma\"\n",
    "]\n",
    "\n",
    "# Define the target directory for copied .txt files\n",
    "copy_target_dir = '../data/input/clean_data'\n",
    "\n",
    "# Ensure the target directory exists\n",
    "os.makedirs(copy_target_dir, exist_ok=True)\n",
    "\n",
    "# Copy all .txt files from the source directories to the target directory\n",
    "for src_dir in source_dirs:\n",
    "    for txt_file in glob.glob(os.path.join(src_dir, \"*.txt\")):\n",
    "        shutil.copy(txt_file, copy_target_dir)\n",
    "\n",
    "# Now, set the source_dir to where we've copied the .txt files\n",
    "source_dir = copy_target_dir\n",
    "\n",
    "# Define the target directory for preprocessing output\n",
    "target_dir = '../data/input/filtered_data'\n",
    "dataset_file = os.path.join(target_dir, \"filtered_dataset.txt\")\n",
    "\n",
    "# Ensure the preprocessing target directory exists\n",
    "os.makedirs(target_dir, exist_ok=True)\n",
    "\n",
    "# Regex pattern to keep English letters, Pali diacritics, and standard punctuation\n",
    "pattern = re.compile(r\"[^a-zA-ZāīūṅñṭḍṇḷĀĪŪṄÑṬḌṆḶ\\s.,!?()-]\")\n",
    "\n",
    "# Function to clean text by removing unwanted characters\n",
    "def clean_text(text):\n",
    "    return pattern.sub('', text)\n",
    "\n",
    "# Find all .txt files in the source directory for preprocessing\n",
    "paths = glob.glob(os.path.join(source_dir, \"*.txt\"))[:1000]  # Limit to first 1000 files\n",
    "\n",
    "# Process and merge content of each file\n",
    "with open(dataset_file, \"w\", encoding='utf-8') as output_file:\n",
    "    for path in paths:\n",
    "        with open(path, \"r\", encoding='utf-8') as file:\n",
    "            for line in file:\n",
    "                cleaned_line = clean_text(line.strip())\n",
    "                if cleaned_line:\n",
    "                    print(cleaned_line, file=output_file)\n",
    "\n",
    "print(\"Corpus loaded and filtered.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean Dataset: Convert to English Sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import spacy\n",
    "import re\n",
    "\n",
    "# Load the English tokenizer, tagger, parser, NER, and word vectors\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def is_likely_english(sentence):\n",
    "    # Check if the sentence has at least one verb and one subject, common in English sentences\n",
    "    has_verb = any(token.pos_ == 'VERB' for token in sentence)\n",
    "    has_subject = any(token.dep_ == 'nsubj' or token.dep_ == 'nsubjpass' for token in sentence)\n",
    "    return has_verb and has_subject\n",
    "\n",
    "def is_not_reference(line):\n",
    "    # Filter out lines that resemble references or citations, e.g., \"SN V,\" \"T II b\"\n",
    "    return not re.search(r'\\b(SN|T|MN|AN|D|SĀ|Ps|Vin|Vism)\\s+[IVXLC]+', line)\n",
    "\n",
    "def filter_sentences(input_file, output_file):\n",
    "    with open(input_file, 'r', encoding='utf-8') as f_in, open(output_file, 'w', encoding='utf-8') as f_out:\n",
    "        for line in f_in:\n",
    "            if is_not_reference(line):\n",
    "                doc = nlp(line.strip())\n",
    "                for sent in doc.sents:\n",
    "                    if is_likely_english(sent) and len(sent.text.split()) > 5:  # Minimum word count check\n",
    "                        f_out.write(sent.text + '\\n')\n",
    "\n",
    "# Specify the path to your input file and the desired output file\n",
    "input_file = '../data/input/filtered_data/filtered_dataset.txt'\n",
    "output_file = '../data/input/filtered_data/cleaned_dataset.txt'\n",
    "\n",
    "# Filter the dataset\n",
    "filter_sentences(input_file, output_file)\n",
    "\n",
    "print(\"Dataset filtering complete. Check the cleaned dataset at:\", output_file)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combined Cleaning and Filtering of Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import os\n",
    "import re\n",
    "import glob\n",
    "import shutil\n",
    "import spacy\n",
    "\n",
    "# Define the source directories for copying .txt files\n",
    "source_dirs = [\n",
    "    \"../data/input/text/books_converted\",\n",
    "    \"../data/input/text/web_txt\",\n",
    "    \"../data/input/audio_transcribed/transcribed_youtube\",\n",
    "    \"../data/input/audio_transcribed/transscribed_audio_dharma\"\n",
    "]\n",
    "\n",
    "# Define the first target directory for copied .txt files\n",
    "copy_target_dir = '../data/input/clean_data'\n",
    "\n",
    "# Ensure the first target directory exists\n",
    "os.makedirs(copy_target_dir, exist_ok=True)\n",
    "\n",
    "# Copy all .txt files from the source directories to the first target directory\n",
    "for src_dir in source_dirs:\n",
    "    for txt_file in glob.glob(os.path.join(src_dir, \"*.txt\")):\n",
    "        shutil.copy(txt_file, copy_target_dir)\n",
    "\n",
    "# Define the target directory for preprocessing output and the output file\n",
    "target_dir = '../data/input/filtered_data'\n",
    "dataset_file = os.path.join(target_dir, \"filtered_dataset.txt\")\n",
    "\n",
    "# Ensure the preprocessing target directory exists\n",
    "os.makedirs(target_dir, exist_ok=True)\n",
    "\n",
    "# Regex pattern to keep English letters, Pali diacritics, and standard punctuation\n",
    "pattern = re.compile(r\"[^a-zA-ZāīūṅñṭḍṇḷĀĪŪṄÑṬḌṆḶ\\s.,!?()-]\")\n",
    "\n",
    "# Function to clean text by removing unwanted characters\n",
    "def clean_text(text):\n",
    "    return pattern.sub('', text)\n",
    "\n",
    "# Find and preprocess .txt files in the first target directory\n",
    "paths = glob.glob(os.path.join(copy_target_dir, \"*.txt\"))[:1000]  # Limit to first 1000 files\n",
    "\n",
    "# Process and merge content of each file\n",
    "with open(dataset_file, \"w\", encoding='utf-8') as output_file:\n",
    "    for path in paths:\n",
    "        with open(path, \"r\", encoding='utf-8') as file:\n",
    "            for line in file:\n",
    "                cleaned_line = clean_text(line.strip())\n",
    "                if cleaned_line:\n",
    "                    print(cleaned_line, file=output_file)\n",
    "\n",
    "# Load the English tokenizer, tagger, parser, NER, and word vectors from spaCy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def is_likely_english(sentence):\n",
    "    # Check if the sentence has at least one verb and one subject\n",
    "    has_verb = any(token.pos_ == 'VERB' for token in sentence)\n",
    "    has_subject = any(token.dep_ == 'nsubj' or token.dep_ == 'nsubjpass' for token in sentence)\n",
    "    return has_verb and has_subject\n",
    "\n",
    "def is_not_reference(line):\n",
    "    # Filter out lines that resemble references or citations\n",
    "    return not re.search(r'\\b(SN|T|MN|AN|D|SĀ|Ps|Vin|Vism)\\s+[IVXLC]+', line)\n",
    "\n",
    "def filter_sentences(input_file, output_file):\n",
    "    with open(input_file, 'r', encoding='utf-8') as f_in, open(output_file, 'w', encoding='utf-8') as f_out:\n",
    "        for line in f_in:\n",
    "            if is_not_reference(line):\n",
    "                doc = nlp(line.strip())\n",
    "                for sent in doc.sents:\n",
    "                    if is_likely_english(sent) and len(sent.text.split()) > 5:  # Minimum word count check\n",
    "                        f_out.write(sent.text + '\\n')\n",
    "\n",
    "# Specify the path for the final output file\n",
    "final_output_file = '../data/input/filtered_data/cleaned_dataset.txt'\n",
    "\n",
    "# Filter the preprocessed dataset\n",
    "filter_sentences(dataset_file, final_output_file)\n",
    "\n",
    "print(\"Corpus loaded, filtered, and cleaned. Check the cleaned dataset at:\", final_output_file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Produce Descriptive Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Words: 2_108_767\n",
      "Unique Words: 26_120\n",
      "Average Word Length: 4.18 characters\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def format_number(number):\n",
    "    \"\"\"Format the number with underscores between hundreds.\"\"\"\n",
    "    return f\"{number:,}\".replace(\",\", \"_\")\n",
    "\n",
    "def count_words(file_path):\n",
    "    \"\"\"Count words in a file and provide descriptive statistics.\"\"\"\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            text = file.read()\n",
    "            # Tokenize the text by non-alphanumeric characters\n",
    "            words = re.findall(r'\\b\\w+\\b', text.lower())\n",
    "            word_count = len(words)\n",
    "            unique_words = len(set(words))\n",
    "            average_word_length = sum(len(word) for word in words) / word_count\n",
    "\n",
    "            print(f\"Total Words: {format_number(word_count)}\")\n",
    "            print(f\"Unique Words: {format_number(unique_words)}\")\n",
    "            print(f\"Average Word Length: {average_word_length:.2f} characters\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"The file at {file_path} was not found.\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    file_path = \"../data/input/filtered_data_open_source/cleaned_filtered_corrected_dataset.txt\"\n",
    "    count_words(file_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 112985 examples [00:00, 446730.44 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 112985\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "target_dir = '../data/input/filtered_data_open_source/'\n",
    "dataset_file = os.path.join(target_dir, \"cleaned_filtered_corrected_dataset.txt\")\n",
    "\n",
    "raw_datasets = load_dataset(\"text\", data_files=dataset_file)\n",
    "raw_datasets\n",
    "# 333_056 lines\n",
    "# previously around 43_000 lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': \"Oh yes, one of the things which is said also is that it's a pure bright mind.\"}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_datasets[\"train\"][666]\n",
    "# with HuggingFace datasets, every sample is a dictionary\n",
    "# a sample from the dataset here is a paragraph\n",
    "# the key is always \"text\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create empty tokenizer and its trainer\n",
    "tokenizer = Tokenizer(BPE(unk_token=\"[UNK]\")) # subword tokenization and ways to merge them\n",
    "trainer = BpeTrainer(vocab_size=5_000, special_tokens=[\"[UNK]\", \"[PAD]\"])\n",
    "# separates the tokens with a space\n",
    "tokenizer.pre_tokenizer = Whitespace()\n",
    "\n",
    "# Batch samples to speed up process\n",
    "def batch_iterator(batch_size=1000):\n",
    "    # the batch size is the number of samples that will be processed at once\n",
    "    # the iterator will yield a batch of samples\n",
    "    # yield is a keyword in Python that is used like return, except the function will return a generator\n",
    "    # a generator is an iterator that generates one item at a time\n",
    "    for i in range(0, len(raw_datasets[\"train\"]), batch_size):\n",
    "        yield raw_datasets[\"train\"][i : i + batch_size][\"text\"]\n",
    "\n",
    "# Train the tokenizer\n",
    "tokenizer.train_from_iterator(batch_iterator(), trainer=trainer, length=len(raw_datasets[\"train\"]))\n",
    "\n",
    "# Saves the tokenizer\n",
    "# when downloading model, we download model, and the tokenizer\n",
    "tokenizer.save(\"tokenizer.json\")\n",
    "\n",
    "# Load it fast\n",
    "# speeds up the process\n",
    "tokenizer = PreTrainedTokenizerFast(tokenizer_file=\"tokenizer.json\") \n",
    "tokenizer.add_special_tokens({\"pad_token\": \"[PAD]\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenize the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 112985/112985 [00:04<00:00, 25023.85 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# start with sequence length of 256\n",
    "# pads the sequences to the same length\n",
    "sequence_length = 256\n",
    "\n",
    "# takes a dictionary as input\n",
    "def tokenize_function(example):\n",
    "    # tokenize the text\n",
    "    tokenized_example = tokenizer(\n",
    "        example[\"text\"],\n",
    "        truncation=True,\n",
    "        padding=True,\n",
    "        max_length=sequence_length,\n",
    "    )\n",
    "    return {\"input_ids\": tokenized_example[\"input_ids\"]}\n",
    "\n",
    "# tokenize entire dataset\n",
    "tokenized_datasets = raw_datasets.map(tokenize_function, batched=True, remove_columns=raw_datasets[\"train\"].column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data collator is used to batch the samples together\n",
    "# data pump for training\n",
    "# collate means to collect and combine\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2LMHeadModel(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(5000, 512)\n",
       "    (wpe): Embedding(256, 512)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-5): 6 x GPT2Block(\n",
       "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=512, out_features=5000, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_config = GPT2Config(\n",
    "    vocab_size=tokenizer.vocab_size, # the size of the vocabulary\n",
    "    pad_token_id=tokenizer.pad_token_id, # the token id for padding\n",
    "    n_ctx=sequence_length, # context length\n",
    "    n_positions=sequence_length, # positions in context, the order of the tokens in sequence\n",
    "    n_embd=512, # embedding dimension\n",
    "    n_head=8, # number of heads in the multi-head attention models\n",
    "    n_layer=6, # number of layers\n",
    ")\n",
    "\n",
    "model = GPT2LMHeadModel(model_config)\n",
    "model\n",
    "\n",
    "# wte: word token embeddings; which means the embeddings of the tokens\n",
    "# wpe: word position embeddings; which means the embeddings of the positions of the tokens\n",
    "# drop: dropout\n",
    "# sequence length: 256\n",
    "# dropout layer: 0.1\n",
    "# dropout is a regularization technique; it prevents overfitting\n",
    "# normalisation is done first, which differs to transformers\n",
    "# normalisation means to scale the input to have a mean of 0 and a standard deviation of 1\n",
    "# Conv1D means 1D convolution; convolutions are used to extract features from the input\n",
    "# LayerNorm means layer normalisation; normalisation is used to improve the training of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/21730 [00:00<?, ?it/s]You're using a PreTrainedTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "  2%|▏         | 500/21730 [04:14<3:00:24,  1.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 5.5573, 'learning_rate': 4.884951679705477e-05, 'epoch': 0.23}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▍         | 1000/21730 [08:31<2:58:32,  1.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 4.9472, 'learning_rate': 4.769903359410953e-05, 'epoch': 0.46}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 1500/21730 [12:56<2:54:48,  1.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 4.7556, 'learning_rate': 4.6548550391164294e-05, 'epoch': 0.69}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|▉         | 2000/21730 [17:17<2:55:14,  1.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 4.6234, 'learning_rate': 4.5398067188219054e-05, 'epoch': 0.92}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 2500/21730 [21:38<2:47:28,  1.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 4.5212, 'learning_rate': 4.424758398527381e-05, 'epoch': 1.15}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▍        | 3000/21730 [26:02<2:43:35,  1.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 4.4332, 'learning_rate': 4.309710078232858e-05, 'epoch': 1.38}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|█▌        | 3500/21730 [30:24<2:39:32,  1.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 4.372, 'learning_rate': 4.194661757938334e-05, 'epoch': 1.61}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█▊        | 4000/21730 [34:47<2:35:23,  1.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 4.2863, 'learning_rate': 4.0796134376438105e-05, 'epoch': 1.84}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 21%|██        | 4500/21730 [39:10<2:31:02,  1.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 4.2371, 'learning_rate': 3.964565117349287e-05, 'epoch': 2.07}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 23%|██▎       | 5000/21730 [43:33<2:26:44,  1.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 4.1697, 'learning_rate': 3.849516797054763e-05, 'epoch': 2.3}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 5500/21730 [47:57<2:22:45,  1.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 4.1276, 'learning_rate': 3.73446847676024e-05, 'epoch': 2.53}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|██▊       | 6000/21730 [52:21<2:18:24,  1.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 4.0946, 'learning_rate': 3.619420156465716e-05, 'epoch': 2.76}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|██▉       | 6500/21730 [56:47<2:13:56,  1.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 4.0727, 'learning_rate': 3.504371836171192e-05, 'epoch': 2.99}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|███▏      | 7000/21730 [1:01:12<2:09:32,  1.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.9865, 'learning_rate': 3.389323515876668e-05, 'epoch': 3.22}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|███▍      | 7500/21730 [1:05:37<2:05:03,  1.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.9674, 'learning_rate': 3.274275195582145e-05, 'epoch': 3.45}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 37%|███▋      | 8000/21730 [1:10:02<1:59:57,  1.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.9536, 'learning_rate': 3.159226875287621e-05, 'epoch': 3.68}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 39%|███▉      | 8500/21730 [1:14:30<1:55:23,  1.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.9329, 'learning_rate': 3.0441785549930974e-05, 'epoch': 3.91}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 41%|████▏     | 9000/21730 [1:18:56<3:00:25,  1.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.8673, 'learning_rate': 2.9291302346985733e-05, 'epoch': 4.14}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|████▎     | 9500/21730 [1:23:26<1:48:03,  1.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.8375, 'learning_rate': 2.81408191440405e-05, 'epoch': 4.37}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 46%|████▌     | 10000/21730 [1:27:56<1:43:18,  1.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.8356, 'learning_rate': 2.6990335941095262e-05, 'epoch': 4.6}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 48%|████▊     | 10500/21730 [1:32:22<1:39:25,  1.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.8149, 'learning_rate': 2.583985273815002e-05, 'epoch': 4.83}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 51%|█████     | 11000/21730 [1:36:50<1:35:02,  1.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.7936, 'learning_rate': 2.4689369535204788e-05, 'epoch': 5.06}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 53%|█████▎    | 11500/21730 [1:41:20<1:30:24,  1.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.7373, 'learning_rate': 2.353888633225955e-05, 'epoch': 5.29}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|█████▌    | 12000/21730 [1:45:50<1:26:05,  1.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.7402, 'learning_rate': 2.2388403129314313e-05, 'epoch': 5.52}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 58%|█████▊    | 12500/21730 [1:50:18<1:21:10,  1.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.7307, 'learning_rate': 2.1237919926369076e-05, 'epoch': 5.75}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|█████▉    | 13000/21730 [1:54:45<1:16:37,  1.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.7126, 'learning_rate': 2.008743672342384e-05, 'epoch': 5.98}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|██████▏   | 13500/21730 [1:59:17<1:12:09,  1.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.6604, 'learning_rate': 1.89369535204786e-05, 'epoch': 6.21}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 64%|██████▍   | 14000/21730 [2:03:45<1:10:23,  1.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.6596, 'learning_rate': 1.7786470317533364e-05, 'epoch': 6.44}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 14500/21730 [2:08:11<1:03:24,  1.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.6476, 'learning_rate': 1.6635987114588127e-05, 'epoch': 6.67}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 69%|██████▉   | 15000/21730 [2:12:38<59:15,  1.89it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.6474, 'learning_rate': 1.548550391164289e-05, 'epoch': 6.9}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 71%|███████▏  | 15500/21730 [2:17:04<54:41,  1.90it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.6112, 'learning_rate': 1.4335020708697653e-05, 'epoch': 7.13}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 74%|███████▎  | 16000/21730 [2:21:32<50:08,  1.90it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.5954, 'learning_rate': 1.3184537505752417e-05, 'epoch': 7.36}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 76%|███████▌  | 16500/21730 [2:25:58<45:46,  1.90it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.5909, 'learning_rate': 1.203405430280718e-05, 'epoch': 7.59}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 78%|███████▊  | 17000/21730 [2:30:22<41:14,  1.91it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.5847, 'learning_rate': 1.0883571099861943e-05, 'epoch': 7.82}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 81%|████████  | 17500/21730 [2:34:47<36:59,  1.91it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.5786, 'learning_rate': 9.733087896916706e-06, 'epoch': 8.05}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 83%|████████▎ | 18000/21730 [2:39:11<32:31,  1.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.5445, 'learning_rate': 8.582604693971469e-06, 'epoch': 8.28}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 85%|████████▌ | 18500/21730 [2:43:37<28:13,  1.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.5427, 'learning_rate': 7.432121491026231e-06, 'epoch': 8.51}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 87%|████████▋ | 19000/21730 [2:48:01<23:50,  1.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.5417, 'learning_rate': 6.281638288080994e-06, 'epoch': 8.74}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|████████▉ | 19500/21730 [2:52:26<19:26,  1.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.5358, 'learning_rate': 5.131155085135757e-06, 'epoch': 8.97}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 92%|█████████▏| 20000/21730 [2:56:51<15:06,  1.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.5035, 'learning_rate': 3.980671882190521e-06, 'epoch': 9.2}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 94%|█████████▍| 20500/21730 [3:01:15<10:44,  1.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.5101, 'learning_rate': 2.830188679245283e-06, 'epoch': 9.43}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 97%|█████████▋| 21000/21730 [3:05:40<06:21,  1.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.5081, 'learning_rate': 1.6797054763000461e-06, 'epoch': 9.66}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 99%|█████████▉| 21500/21730 [3:10:02<02:00,  1.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.4954, 'learning_rate': 5.29222273354809e-07, 'epoch': 9.89}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 21730/21730 [3:12:03<00:00,  1.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 11523.8369, 'train_samples_per_second': 98.045, 'train_steps_per_second': 1.886, 'train_loss': 3.922839495908594, 'epoch': 10.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# clear VRAM GPU cache\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "output_path = \"output\"\n",
    "\n",
    "# Create the Trainer\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=output_path, # output directory\n",
    "    overwrite_output_dir=True, # overwrite the content of the output directory\n",
    "    num_train_epochs=10, # number of training epochs\n",
    "    #per_device_train_batch_size=16, # batch size for training per device (e.g. multiple GPUs), which took 8 minutes\n",
    "    #per_device_train_batch_size=32 # double, because i was only using around 3GB of VRAM, which took 7.5 minutes\n",
    "    per_device_train_batch_size=52 # increase, because i was only using around 6GB of VRAM with 32, which also took 7.5 minutes\n",
    "    # now uses 7704MiB of VRAM\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model, # the model\n",
    "    args=training_args, # training arguments\n",
    "    data_collator=data_collator, # data collator\n",
    "    train_dataset=tokenized_datasets[\"train\"] # training dataset\n",
    ")\n",
    "\n",
    "# Train\n",
    "trainer.train()\n",
    "\n",
    "# Save\n",
    "tokenizer.save_pretrained(output_path)\n",
    "model.save_pretrained(output_path)\n",
    "\n",
    "# TODO:\n",
    "# You're using a PreTrainedTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[483, 349,  80]], device='cuda:0')\n",
      "Jhana meditation is what we call jhana. Jhana. because of the jhana factors are so strong and we can see that the jhana is very still and the first jhana is not the first jhana the third jhana is very strong and the second jhana is the first jhana and the fourth jhana and the second jhana is neither pleasant and the one pointedness that is neither pleasant nor not the piti and then the first jhana so there is no longer the first jhana is an object what it is one is at the third jhana which\n"
     ]
    }
   ],
   "source": [
    "# Encode the conditioning tokens.\n",
    "input_ids = tokenizer.encode(\"Jhana meditation is \", return_tensors=\"pt\").cuda()\n",
    "print(input_ids)\n",
    "\n",
    "# Generate more tokens.\n",
    "generated_ids = model.generate(\n",
    "    input_ids,\n",
    "    max_length=100,\n",
    "    do_sample=True,\n",
    "    temperature=0.5\n",
    ")\n",
    "generated_sequence = tokenizer.decode(generated_ids[0], clean_up_tokenization_spaces=True)\n",
    "print(generated_sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1442,  651,  766,  322,    4,  104]], device='cuda:0')\n",
      "To attain access concentration, we can experience the breath. as we breathe in, breathing out, and the breath is the breath, and the breath is the breath, the breath, the breath, the breath is the breath, the breath, the breath, the breath, the breath, the breath, the breath, the breath, the breath, the breath, the breath. the breath, the breath, the breath, the breath, the breath, the breath, the breath, the breath, the\n"
     ]
    }
   ],
   "source": [
    "# Encode the conditioning tokens.\n",
    "input_ids = tokenizer.encode(\"To attain access concentration, we \", return_tensors=\"pt\").cuda()\n",
    "print(input_ids)\n",
    "\n",
    "# Generate more tokens.\n",
    "generated_ids = model.generate(\n",
    "    input_ids,\n",
    "    max_length=100,\n",
    "    do_sample=True,\n",
    "    temperature=0.5\n",
    ")\n",
    "generated_sequence = tokenizer.decode(generated_ids[0], clean_up_tokenization_spaces=True)\n",
    "print(generated_sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[483, 349,  80]], device='cuda:0')\n",
      "Jhana meditation is a very helpful way to practice..... will be used to be a word., and then you've been talking about the word for jhana. or the jhana............................, and then you have to have to be able to have a sense of things and come from the different things. of the Jhana. and then you have to be\n"
     ]
    }
   ],
   "source": [
    "# Assuming 'tokenizer' and 'model' are already defined\n",
    "\n",
    "# Move model to GPU\n",
    "model = model.cuda()\n",
    "\n",
    "# Encode the conditioning tokens and move them to GPU\n",
    "input_ids = tokenizer.encode(\"Jhana meditation is \", return_tensors=\"pt\").cuda()\n",
    "print(input_ids)\n",
    "\n",
    "# Generate more tokens\n",
    "generated_ids = model.generate(\n",
    "    input_ids,\n",
    "    max_length=100,\n",
    "    do_sample=True,\n",
    "    temperature=0.5\n",
    ")\n",
    "\n",
    "# Decode generated ids\n",
    "generated_sequence = tokenizer.decode(generated_ids[0], clean_up_tokenization_spaces=True)\n",
    "print(generated_sequence)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1442,  243,  257,  349,    4,  104,  332]], device='cuda:0')\n",
      "To practice jhana meditation, we sit in a very deep meditation, in a way, in a way, in a way that we can see the whole world, as a way that we can see, we can see things., and see them as they are, and and they all are, and I can get to them, and they can be a little more, but the way we can see the way through the way that way we can go through the way it and the way you know it\n"
     ]
    }
   ],
   "source": [
    "# Encode the conditioning tokens.\n",
    "input_ids = tokenizer.encode(\"To practice jhana meditation, we sit \", return_tensors=\"pt\").cuda()\n",
    "print(input_ids)\n",
    "\n",
    "# Generate more tokens.\n",
    "generated_ids = model.generate(\n",
    "    input_ids,\n",
    "    max_length=100,\n",
    "    do_sample=True,\n",
    "    temperature=0.5\n",
    ")\n",
    "generated_sequence = tokenizer.decode(generated_ids[0], clean_up_tokenization_spaces=True)\n",
    "print(generated_sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input text: In the first jhana, the experience is \n",
      "Encoded input_ids: tensor([[529,  71, 315, 257,   4,  71, 342,  80]], device='cuda:0') on device cuda:0\n",
      "Raw generated ids: tensor([[ 529,   71,  315,  257,    4,   71,  342,   80,   71,  521,  552,   87,\n",
      "           71,  315,  257,    4,   71,  521,  257,    4,   71,  686,  257,    4,\n",
      "           85,   79,    3,   52,   71,  139,    5, 1942,   87,   71,  164,    6,\n",
      "            6,  272,   80, 2997,  307,  121,  924,   85,  528,  909,   87,  322,\n",
      "            6,  568,    6,  778,    6,    4,  272,   80, 2997,  307,  881,   85,\n",
      "          528,    6,    6,    6,    6,   87, 1857,    6,  778,    6,    6,    6,\n",
      "            6,  142,   71,  315,  257,    6,  698,   87,   71,  699,  483,    6,\n",
      "            6,   87,   71,  521,  257,    6,  909,   87,  322,    6,    6,    6,\n",
      "            6,    6,    6,   85]], device='cuda:0')\n",
      "Decoded generated sequence: In the first jhana, the experience is the second factor of the first jhana, the second jhana, the third jhana, and it's the one - pointedness of the mind.. which is accompanied by this rapture and happiness born of concentration. mindfulness. sukkha., which is accompanied by equanimity and happiness.... of seclusion. sukkha.... for the first jhana. factors of the fourth Jhana.. of the second jhana. born of concentration...... and\n"
     ]
    }
   ],
   "source": [
    "# Assuming 'tokenizer' and 'model' are already defined\n",
    "\n",
    "# Move model to GPU\n",
    "model = model.cuda()\n",
    "\n",
    "# Encode the conditioning tokens and move them to GPU\n",
    "input_text = \"In the first jhana, the experience is \"\n",
    "input_ids = tokenizer.encode(input_text, return_tensors=\"pt\").cuda()\n",
    "print(f\"Input text: {input_text}\")\n",
    "print(f\"Encoded input_ids: {input_ids} on device {input_ids.device}\")\n",
    "\n",
    "# Generate more tokens\n",
    "generated_ids = model.generate(\n",
    "    input_ids,\n",
    "    max_length=100,\n",
    "    do_sample=True,\n",
    "    temperature=0.5\n",
    ")\n",
    "print(f\"Raw generated ids: {generated_ids}\")\n",
    "\n",
    "# Decode generated ids\n",
    "generated_sequence = tokenizer.decode(generated_ids[0], clean_up_tokenization_spaces=True)\n",
    "print(f\"Decoded generated sequence: {generated_sequence}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input text: 'Jhana meditation is '\n"
     ]
    }
   ],
   "source": [
    "input_text = \"Jhana meditation is \"\n",
    "print(f\"Input text: '{input_text}'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens: ['Jhana', 'meditation', 'is']\n"
     ]
    }
   ],
   "source": [
    "tokens = tokenizer.tokenize(input_text)\n",
    "print(f\"Tokens: {tokens}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded input_ids: tensor([[483, 349,  80]])\n",
      "Decoded tokens from input_ids: ['Jhana', 'meditation', 'is']\n"
     ]
    }
   ],
   "source": [
    "input_ids = tokenizer.encode(input_text, return_tensors=\"pt\")\n",
    "print(f\"Encoded input_ids: {input_ids}\")\n",
    "decoded_tokens = [tokenizer.decode([id]) for id in input_ids[0]]\n",
    "print(f\"Decoded tokens from input_ids: {decoded_tokens}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tokens: 3\n",
      "Number of input_ids: 3\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of tokens: {len(tokens)}\")\n",
    "print(f\"Number of input_ids: {input_ids.shape[1]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Special tokens added: ['Jhana', 'meditation', 'is']\n"
     ]
    }
   ],
   "source": [
    "print(f\"Special tokens added: {tokenizer.build_inputs_with_special_tokens(tokens)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention mask: tensor([[1, 1, 1]])\n"
     ]
    }
   ],
   "source": [
    "attention_mask = tokenizer(input_text, return_tensors=\"pt\")[\"attention_mask\"]\n",
    "print(f\"Attention mask: {attention_mask}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model device: cuda:0\n",
      "Input ids device: cpu\n"
     ]
    }
   ],
   "source": [
    "print(f\"Model device: {next(model.parameters()).device}\")\n",
    "print(f\"Input ids device: {input_ids.device}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jhana meditation is used to be translated in English as the sutras. and the Buddha says, and it's a very interesting thing., it's very important to say, it's a very, it's just a kind of a very, very subtle, very still, very, very, very, very subtle, very, very, very, very, very, very kind of a very, very subtle, very, very, very, very, very, very useful thing is\n"
     ]
    }
   ],
   "source": [
    "# Ensure input_ids are on the same device as the model\n",
    "input_ids = input_ids.to(next(model.parameters()).device)\n",
    "\n",
    "# Now you can proceed with generating more tokens\n",
    "generated_ids = model.generate(\n",
    "    input_ids,\n",
    "    max_length=100,\n",
    "    do_sample=True,\n",
    "    temperature=0.5\n",
    ")\n",
    "\n",
    "# Decode generated ids\n",
    "generated_sequence = tokenizer.decode(generated_ids[0], clean_up_tokenization_spaces=True)\n",
    "print(generated_sequence)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model device: cuda:0\n",
      "Input ids device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "print(f\"Model device: {next(model.parameters()).device}\")\n",
    "print(f\"Input ids device: {input_ids.device}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model locally\n",
    "\n",
    "#model = GPT2LMHeadModel.from_pretrained(pretrained_model_name_or_path=path)\n",
    "#tokenizer = PreTrainedTokenizerFast(pretrained_model_name_or_path=path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jhana_ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
